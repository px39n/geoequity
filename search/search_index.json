{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"GeoEquity","text":"<p>Spatial Equity Assessment for Machine Learning Models</p> <p>GeoEquity diagnoses and visualizes spatial performance disparities in geospatial ML models\u2014identifying where models underperform and predicting accuracy across space.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>\ud83c\udfaf Two-Stage Accuracy Prediction: Density effect (GAM) + Spatial residual (SVM)</li> <li>\ud83d\udcca Spatial Equity Diagnostics: Identify under-served regions</li> <li>\ud83d\uddfa\ufe0f Visualization Tools: Publication-ready accuracy maps</li> <li>\ud83d\udcc8 Model Comparison: Benchmark against baselines</li> </ul>"},{"location":"#quick-example","title":"Quick Example","text":"<pre><code>from geoequity import TwoStageModel\n\n# Fit on your validated predictions\nts = TwoStageModel()\nts.fit(df_test, model_name='mymodel')\n\n# Predict accuracy anywhere\naccuracy = ts.predict(longitude=5.0, latitude=50.0, density=0.001)\n\n# Generate diagnostics\nts.diagnose(save_dir='diagnostics/')\n</code></pre>"},{"location":"#required-data","title":"Required Data","text":"<p>Input: <code>df_test</code> should be a test set DataFrame (out-of-sample predictions) containing:</p> Column Required Description <code>longitude</code>, <code>latitude</code> \u2713 Spatial coordinates <code>observed</code> \u2713 Ground truth values <code>predicted_{model_name}</code> \u2713 Model predictions (name must match <code>model_name</code> parameter) <code>density</code> \u2713 Spatial data density (user-defined function) <code>sufficiency</code> Optional* Training sample size per prediction <p>*Note on optional fields:</p> <ul> <li> <p><code>density</code>: Any user-defined spatial density metric (kernel density, k-NN, IDW, etc.)   <pre><code>df['density'] = gaussian_kernel(coords, bandwidth=1.0)  # Example\n</code></pre></p> </li> <li> <p><code>sufficiency</code>: Optional if all samples share the same training size (auto-detected). Required for k-fold CV.   <pre><code>df['sufficiency'] = 1000000  # Single value\n# OR\npd.concat([df1.assign(sufficiency=800000), df2.assign(sufficiency=1000000)])  # Multiple\n</code></pre></p> </li> </ul>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install geoequity\n</code></pre>"},{"location":"#documentation","title":"Documentation","text":"Section Description Quick Start Data preparation + basic usage Two-Stage Model Core methodology Model Comparison Benchmark methods Visualization Accuracy maps"},{"location":"#example-notebooks","title":"Example Notebooks","text":"Notebook Description Standard Workflow Complete pipeline Framework Comparison Compare baselines Scientific Visualization Publication maps"},{"location":"#citation","title":"Citation","text":"<pre><code>@article{liang2025geoequity,\n  title={Countering Local Overfitting for Equitable Spatiotemporal Modeling},\n  author={Liang, Zhehao and Castruccio, Stefano and Crippa, Paola},\n  journal={...},\n  year={2025}\n}\n</code></pre>"},{"location":"#license","title":"License","text":"<p>MIT License</p>"},{"location":"api/data/","title":"Data Utilities","text":""},{"location":"api/data/#split_test_train","title":"split_test_train","text":"<p>Split data into train/test sets with spatial awareness.</p>"},{"location":"api/data/#usage","title":"Usage","text":"<pre><code>from geoequity.data import split_test_train\n\n# Single split\ntrain_sites, test_sites, train_idx, test_idx, df = split_test_train(\n    df, split=0.2, flag='Site', seed=42\n)\n\n# Cross-validation\ntrain_list, test_list, train_idx_list, test_idx_list, df = split_test_train(\n    df, cv=5, flag='Site', seed=42\n)\n</code></pre>"},{"location":"api/data/#parameters","title":"Parameters","text":"Name Type Default Description df DataFrame - Input data split float 0.2 Test set fraction flag str 'Site' Splitting strategy: 'Site', 'Grid', 'Spatiotemporal' cv int None Number of CV folds (overrides split) seed int 42 Random seed grid_size int 10 Grid cells per dimension (for Grid flag)"},{"location":"api/data/#calculate_density","title":"calculate_density","text":"<p>Calculate data density for each observation based on nearby points.</p>"},{"location":"api/data/#usage_1","title":"Usage","text":"<pre><code>from geoequity.data import calculate_density\n\ndf = calculate_density(df, radius=500)\n</code></pre>"},{"location":"api/data/#parameters_1","title":"Parameters","text":"Name Type Default Description df DataFrame - Input data with lon/lat columns radius float 500 Search radius in kilometers lon_col str 'longitude' Longitude column name lat_col str 'latitude' Latitude column name"},{"location":"api/data/#returns","title":"Returns","text":"<p>DataFrame with added <code>density</code> column.</p>"},{"location":"api/two-stage/","title":"TwoStageModel","text":"<p>The core model for predicting spatial variations in ML model accuracy.</p> <pre><code>from geoequity import TwoStageModel\n\nmodel = TwoStageModel(spline=7, lam=0.5, resolution=[30, 30])\n</code></pre>"},{"location":"api/two-stage/#constructor","title":"Constructor","text":"Parameter Type Default Description spline int 7 Number of spline bases for GAM lam float 0.5 GAM regularization parameter resolution list [30, 30] Grid resolution for SVM"},{"location":"api/two-stage/#methods","title":"Methods","text":""},{"location":"api/two-stage/#fit","title":"fit","text":"<pre><code>def fit(self, df, model_col, target_col='observed', \n        density_col='density', lon_col='longitude', lat_col='latitude'):\n</code></pre> <p>Train the two-stage model on test data.</p> <p>Parameters:</p> Name Type Description df DataFrame Test data with predictions and ground truth model_col str Column name containing model predictions target_col str Column name containing ground truth values density_col str Column name containing density values lon_col str Column name for longitude lat_col str Column name for latitude"},{"location":"api/two-stage/#predict","title":"predict","text":"<pre><code>def predict(self, longitude, latitude, density):\n</code></pre> <p>Predict accuracy for given coordinates and density.</p> <p>Parameters:</p> Name Type Description longitude float or array Longitude(s) to predict latitude float or array Latitude(s) to predict density float or array Density value(s) <p>Returns: Predicted R\u00b2 score(s)</p>"},{"location":"api/two-stage/#diagnose","title":"diagnose","text":"<pre><code>def diagnose(self, save_dir='diagnostics/', show=True):\n</code></pre> <p>Generate diagnostic plots and summary report.</p> <p>Parameters:</p> Name Type Description save_dir str Directory to save outputs show bool Whether to display plots <p>Outputs: - <code>stage1_gam.png</code>: GAM partial dependence plots - <code>stage2_svm.png</code>: Spatial residual map - <code>diagnosis.txt</code>: Summary statistics</p>"},{"location":"api/visualization/","title":"Visualization","text":""},{"location":"api/visualization/#plot_accuracy_map","title":"plot_accuracy_map","text":"<p>Plot predicted accuracy as a spatial map.</p>"},{"location":"api/visualization/#usage","title":"Usage","text":"<pre><code>from geoequity.visualization import plot_accuracy_map\n\nfig, ax = plot_accuracy_map(\n    model,\n    ds,\n    mask=land_mask,\n    cmap='Spectral_r',\n    vmin=0, vmax=1\n)\n</code></pre>"},{"location":"api/visualization/#parameters","title":"Parameters","text":"Name Type Default Description model TwoStageModel - Fitted TwoStageModel ds xarray.Dataset - Dataset with lon/lat coordinates mask array None Boolean mask for valid regions cmap str 'Spectral_r' Matplotlib colormap vmin float 0 Colorbar minimum vmax float 1 Colorbar maximum"},{"location":"api/visualization/#plot_accuracy_comparison","title":"plot_accuracy_comparison","text":"<p>Compare accuracy across multiple visualization modes.</p>"},{"location":"api/visualization/#usage_1","title":"Usage","text":"<pre><code>from geoequity.visualization import plot_accuracy_comparison\n\nfig, axes = plot_accuracy_comparison(\n    df,\n    model_name='MyModel',\n    modes=['observation', 'interpolation', 'spatial_model'],\n    accuracy_range=(0, 1),\n    lon_range=(-10, 35),\n    lat_range=(35, 70)\n)\n</code></pre>"},{"location":"api/visualization/#parameters_1","title":"Parameters","text":"Name Type Default Description df DataFrame - Data with model predictions model_name str - Model column prefix modes list - Visualization modes to compare accuracy_range tuple (0, 1) Colorbar range lon_range tuple None Longitude bounds lat_range tuple None Latitude bounds"},{"location":"examples/Framework_Comparision/","title":"Baseline Model Comparison","text":"In\u00a0[1]: Copied! <pre>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\nimport sys\nfrom pathlib import Path\n%load_ext autoreload\n%autoreload 2\n\n# Add geoequity to path\nGEOEQUITY_ROOT = Path(\"..\").resolve()\nif str(GEOEQUITY_ROOT) not in sys.path:\n    sys.path.insert(0, str(GEOEQUITY_ROOT))\n\n# Import geoequity modules\nfrom evaluation import eval_baseline_comparison\nfrom data import calculate_density, split_test_train\nfrom models import SpatialRegressor, InterpolationModel\n\nprint(\"Ready for baseline comparison!\")\n</pre> import numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn.linear_model import LinearRegression from sklearn.metrics import r2_score import sys from pathlib import Path %load_ext autoreload %autoreload 2  # Add geoequity to path GEOEQUITY_ROOT = Path(\"..\").resolve() if str(GEOEQUITY_ROOT) not in sys.path:     sys.path.insert(0, str(GEOEQUITY_ROOT))  # Import geoequity modules from evaluation import eval_baseline_comparison from data import calculate_density, split_test_train from models import SpatialRegressor, InterpolationModel  print(\"Ready for baseline comparison!\")  <pre>Ready for baseline comparison!\n</pre> In\u00a0[2]: Copied! <pre># Load data - replace with your prediction data\nDATA_PATH = 'data/df_example.pkl'\nTARGET_COL = 'Ozone'\nPRED_COL = 'predicted_linear'\n\ndf = pd.read_pickle(DATA_PATH)\ndf['time'] = pd.to_datetime(df['time'])\n\n# Sample for demo (remove for full analysis)\nn_sample = min(200000, len(df))\ndf = df.sample(n=n_sample, random_state=42).copy()\ndf['sufficiency'] = len(df)\n\n# Calculate density\ndf = calculate_density(df, radius=500)\n\n# Train a simple model for demo\nfrom data.feature import simple_feature_engineering\n_, _, train_idx, test_idx, df = split_test_train(df, split=0.2, flag='Site', seed=42, verbose=0)\n\nFEATURE_COLS = [c for c in ['TROPOMI_ozone', 'OMI_ozone', 'tco3', 'blh', 't2m', 'latitude', 'longitude'] if c in df.columns]\nX, y, _ = simple_feature_engineering(df, FEATURE_COLS, TARGET_COL, standardize=True, verbose=False)\n\nmodel = LinearRegression()\nmodel.fit(X.loc[train_idx], y.loc[train_idx])\ndf[PRED_COL] = model.predict(X)\ndf['observed'] = df[TARGET_COL]\n\n# Use test data\ndf_analysis = df.loc[test_idx].copy()\nprint(f\"Data: {len(df_analysis):,} samples\")\nprint(f\"Density range: [{df_analysis['density'].min():.2e}, {df_analysis['density'].max():.2e}]\")\n</pre> # Load data - replace with your prediction data DATA_PATH = 'data/df_example.pkl' TARGET_COL = 'Ozone' PRED_COL = 'predicted_linear'  df = pd.read_pickle(DATA_PATH) df['time'] = pd.to_datetime(df['time'])  # Sample for demo (remove for full analysis) n_sample = min(200000, len(df)) df = df.sample(n=n_sample, random_state=42).copy() df['sufficiency'] = len(df)  # Calculate density df = calculate_density(df, radius=500)  # Train a simple model for demo from data.feature import simple_feature_engineering _, _, train_idx, test_idx, df = split_test_train(df, split=0.2, flag='Site', seed=42, verbose=0)  FEATURE_COLS = [c for c in ['TROPOMI_ozone', 'OMI_ozone', 'tco3', 'blh', 't2m', 'latitude', 'longitude'] if c in df.columns] X, y, _ = simple_feature_engineering(df, FEATURE_COLS, TARGET_COL, standardize=True, verbose=False)  model = LinearRegression() model.fit(X.loc[train_idx], y.loc[train_idx]) df[PRED_COL] = model.predict(X) df['observed'] = df[TARGET_COL]  # Use test data df_analysis = df.loc[test_idx].copy() print(f\"Data: {len(df_analysis):,} samples\") print(f\"Density range: [{df_analysis['density'].min():.2e}, {df_analysis['density'].max():.2e}]\")  <pre>Calculating density (r=500km):   0%|          | 0/1302 [00:00&lt;?, ?it/s]</pre> <pre>Data: 39,565 samples\nDensity range: [4.32e-06, 2.11e-04]\n</pre> In\u00a0[3]: Copied! <pre># Define models to compare\nmodel_list = ['linear', 'lightgbm', 'svm', 'gam_monotonic', 'interpolation', 'two_stage']\n\nprint(\"Models to compare:\")\nfor m in model_list:\n    print(f\"  - {m}\")\n</pre> # Define models to compare model_list = ['linear', 'lightgbm', 'svm', 'gam_monotonic', 'interpolation', 'two_stage']  print(\"Models to compare:\") for m in model_list:     print(f\"  - {m}\")  <pre>Models to compare:\n  - linear\n  - lightgbm\n  - svm\n  - gam_monotonic\n  - interpolation\n  - two_stage\n</pre> In\u00a0[5]: Copied! <pre># Scenario 1: Unseen Spatial - predict accuracy at new locations\nreport_spatial = eval_baseline_comparison(\n    df_analysis,\n    model_list=model_list,\n    density_bins=30,\n    split_method='spatial',\n    train_by='grid',\n    evaluate_by='grid',\n    metric='correlation',\n    full_features='Spatial'\n)\n</pre> # Scenario 1: Unseen Spatial - predict accuracy at new locations report_spatial = eval_baseline_comparison(     df_analysis,     model_list=model_list,     density_bins=30,     split_method='spatial',     train_by='grid',     evaluate_by='grid',     metric='correlation',     full_features='Spatial' )  <pre>\n============================================================\nEvaluation: SPATIAL split \u2192 GRID evaluate\nFeatures: Spatial | Metric: correlation\n============================================================\nSplit: Train 30,572 | Test 8,993\n</pre> <pre>Evaluating:   0%|          | 0/1 [00:00&lt;?, ?it/s]</pre> <pre>\n================================================================================\nResults (CORRELATION)\n================================================================================\nPrediction       Linear Regre      LightGBM     SVM (RBF)  Monotonic GA  IDW Interpol  TwoStageMode\n---------------------------------------------------------------------------------------------------\nlinear                -0.1160        0.0739        0.5745        0.4010        0.5903        0.6842\n================================================================================\n</pre> In\u00a0[7]: Copied! <pre># Scenario 2: Unseen Sampling - predict accuracy at new density levels\nreport_sampling = eval_baseline_comparison(\n    df_analysis,\n    model_list=model_list,\n    density_bins=30,\n    split_method='sampling',\n    train_by='grid',\n    evaluate_by='sampling',\n    metric='correlation',\n    full_features='Spatial'\n)\n</pre> # Scenario 2: Unseen Sampling - predict accuracy at new density levels report_sampling = eval_baseline_comparison(     df_analysis,     model_list=model_list,     density_bins=30,     split_method='sampling',     train_by='grid',     evaluate_by='sampling',     metric='correlation',     full_features='Spatial' )  <pre>\n============================================================\nEvaluation: SAMPLING split \u2192 SAMPLING evaluate\nFeatures: Spatial | Metric: correlation\n============================================================\nSplit: Train 27,551 | Test 12,014\n</pre> <pre>Evaluating:   0%|          | 0/1 [00:00&lt;?, ?it/s]</pre> <pre>\n================================================================================\nResults (CORRELATION)\n================================================================================\nPrediction       Linear Regre      LightGBM     SVM (RBF)  Monotonic GA  IDW Interpol  TwoStageMode\n---------------------------------------------------------------------------------------------------\nlinear                -0.1404       -0.0121       -0.1961        0.1982        0.1259        0.5168\n================================================================================\n</pre> In\u00a0[8]: Copied! <pre>def plot_comparison(report_spatial, report_sampling, model_list):\n    \"\"\"Visualize comparison results.\"\"\"\n    MODEL_DISPLAY = {\n        'linear': 'Linear\\nRegression',\n        'lightgbm': 'LightGBM',\n        'svm': 'SVM\\n(RBF)',\n        'gam_monotonic': 'Monotonic\\nGAM',\n        'interpolation': 'IDW\\nInterpolation',\n        'two_stage': 'TwoStage\\nModel'\n    }\n    \n    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n    \n    pred_name = list(report_spatial.keys())[0]\n    spatial_scores = [report_spatial[pred_name].get(m, 0) for m in model_list]\n    sampling_scores = [report_sampling[pred_name].get(m, 0) for m in model_list]\n    \n    x = np.arange(len(model_list))\n    labels = [MODEL_DISPLAY.get(m, m) for m in model_list]\n    colors = ['#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7', '#DDA0DD', '#FF6B6B']\n    \n    # Scenario 1: Unseen Spatial\n    bars1 = axes[0].bar(x, spatial_scores, color=colors, edgecolor='white', linewidth=1.5)\n    axes[0].set_ylabel('Correlation (r)', fontsize=12)\n    axes[0].set_title('Unseen Spatial\\n(Predict at new locations)', fontsize=13, fontweight='bold')\n    axes[0].set_xticks(x)\n    axes[0].set_xticklabels(labels, fontsize=9)\n    axes[0].axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n    axes[0].set_ylim(-0.2, 1.0)\n    for bar, score in zip(bars1, spatial_scores):\n        if not np.isnan(score):\n            axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.03,\n                        f'{score:.3f}', ha='center', fontsize=9, fontweight='bold')\n    \n    # Scenario 2: Unseen Sampling\n    bars2 = axes[1].bar(x, sampling_scores, color=colors, edgecolor='white', linewidth=1.5)\n    axes[1].set_ylabel('Correlation (r)', fontsize=12)\n    axes[1].set_title('Unseen Sampling\\n(Predict at new density levels)', fontsize=13, fontweight='bold')\n    axes[1].set_xticks(x)\n    axes[1].set_xticklabels(labels, fontsize=9)\n    axes[1].axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n    axes[1].set_ylim(-0.2, 1.0)\n    for bar, score in zip(bars2, sampling_scores):\n        if not np.isnan(score):\n            axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.03,\n                        f'{score:.3f}', ha='center', fontsize=9, fontweight='bold')\n    \n    plt.tight_layout()\n    plt.show()\n\nplot_comparison(report_spatial, report_sampling, model_list)\n</pre> def plot_comparison(report_spatial, report_sampling, model_list):     \"\"\"Visualize comparison results.\"\"\"     MODEL_DISPLAY = {         'linear': 'Linear\\nRegression',         'lightgbm': 'LightGBM',         'svm': 'SVM\\n(RBF)',         'gam_monotonic': 'Monotonic\\nGAM',         'interpolation': 'IDW\\nInterpolation',         'two_stage': 'TwoStage\\nModel'     }          fig, axes = plt.subplots(1, 2, figsize=(14, 5))          pred_name = list(report_spatial.keys())[0]     spatial_scores = [report_spatial[pred_name].get(m, 0) for m in model_list]     sampling_scores = [report_sampling[pred_name].get(m, 0) for m in model_list]          x = np.arange(len(model_list))     labels = [MODEL_DISPLAY.get(m, m) for m in model_list]     colors = ['#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7', '#DDA0DD', '#FF6B6B']          # Scenario 1: Unseen Spatial     bars1 = axes[0].bar(x, spatial_scores, color=colors, edgecolor='white', linewidth=1.5)     axes[0].set_ylabel('Correlation (r)', fontsize=12)     axes[0].set_title('Unseen Spatial\\n(Predict at new locations)', fontsize=13, fontweight='bold')     axes[0].set_xticks(x)     axes[0].set_xticklabels(labels, fontsize=9)     axes[0].axhline(y=0, color='gray', linestyle='--', alpha=0.5)     axes[0].set_ylim(-0.2, 1.0)     for bar, score in zip(bars1, spatial_scores):         if not np.isnan(score):             axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.03,                         f'{score:.3f}', ha='center', fontsize=9, fontweight='bold')          # Scenario 2: Unseen Sampling     bars2 = axes[1].bar(x, sampling_scores, color=colors, edgecolor='white', linewidth=1.5)     axes[1].set_ylabel('Correlation (r)', fontsize=12)     axes[1].set_title('Unseen Sampling\\n(Predict at new density levels)', fontsize=13, fontweight='bold')     axes[1].set_xticks(x)     axes[1].set_xticklabels(labels, fontsize=9)     axes[1].axhline(y=0, color='gray', linestyle='--', alpha=0.5)     axes[1].set_ylim(-0.2, 1.0)     for bar, score in zip(bars2, sampling_scores):         if not np.isnan(score):             axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.03,                         f'{score:.3f}', ha='center', fontsize=9, fontweight='bold')          plt.tight_layout()     plt.show()  plot_comparison(report_spatial, report_sampling, model_list)"},{"location":"examples/Framework_Comparision/#baseline-model-comparison","title":"Baseline Model Comparison\u00b6","text":"<p>Compare different baseline methods for predicting spatial accuracy patterns.</p>"},{"location":"examples/Framework_Comparision/#two-evaluation-scenarios","title":"Two Evaluation Scenarios\u00b6","text":"Scenario Split Method Evaluate By Question Answered Unseen Spatial spatial grid How well does the model predict accuracy at new locations? Unseen Sampling sampling sampling How well does the model predict accuracy for new density levels?"},{"location":"examples/Framework_Comparision/#models-compared","title":"Models Compared\u00b6","text":"Category Model Description Traditional ML <code>linear</code> Linear Regression <code>svm</code> Support Vector Regression (RBF) <code>lightgbm</code> Gradient Boosting GAM <code>gam_monotonic</code> Monotonic GAM with interaction Interpolation <code>interpolation</code> IDW (Inverse Distance Weighting) Two-Stage <code>two_stage</code> GAM (density) + SVM (spatial residual)"},{"location":"examples/Framework_Comparision/#import","title":"Import\u00b6","text":""},{"location":"examples/Framework_Comparision/#1-load-data","title":"1. Load Data\u00b6","text":"<p>Load your prediction data with <code>observed</code> and <code>predicted_*</code> columns.</p>"},{"location":"examples/Framework_Comparision/#2-run-comparison","title":"2. Run Comparison\u00b6","text":"<p>The <code>eval_baseline_comparison</code> function from <code>geoequity.evaluation</code> handles:</p> <ul> <li>Data splitting (spatial or sampling-based)</li> <li>Model training and evaluation</li> <li>Results aggregation and display</li> </ul>"},{"location":"examples/Framework_Comparision/#3-scenario-1-unseen-spatial","title":"3. Scenario 1: Unseen Spatial\u00b6","text":"<p>Question: How well can different methods predict accuracy at new spatial locations?</p> <ul> <li>Split: By spatial grid (70% train / 30% test grids)</li> <li>Evaluate: On held-out spatial grids</li> <li>Features: Spatial only (longitude, latitude)</li> </ul>"},{"location":"examples/Framework_Comparision/#4-scenario-2-unseen-sampling","title":"4. Scenario 2: Unseen Sampling\u00b6","text":"<p>Question: How well can different methods predict accuracy for new density levels?</p> <ul> <li>Split: By density bins (70% train / 30% test bins)</li> <li>Evaluate: On held-out density bins</li> <li>Features: Spatial only (longitude, latitude)</li> </ul>"},{"location":"examples/Framework_Comparision/#5-visualize-comparison","title":"5. Visualize Comparison\u00b6","text":""},{"location":"examples/Framework_Comparision/#summary","title":"Summary\u00b6","text":""},{"location":"examples/Framework_Comparision/#method-categories","title":"Method Categories\u00b6","text":"Category Method Key Characteristics Spatial Regression Linear, SVM, LightGBM Directly regress on spatial coordinates Interpolation IDW Weighted average of nearby observations GAM Monotonic GAM Captures non-linear, monotonic relationships Two-Stage TwoStageModel Combines density effect (GAM) + spatial residual (SVM)"},{"location":"examples/Framework_Comparision/#key-insights","title":"Key Insights\u00b6","text":"<ol> <li><p>Unseen Spatial: How well can we predict accuracy at new locations?</p> <ul> <li>Interpolation methods (IDW) leverage spatial autocorrelation</li> <li>TwoStageModel captures both global density effect and local spatial patterns</li> </ul> </li> <li><p>Unseen Sampling: How well can we predict accuracy at new density levels?</p> <ul> <li>GAM excels at capturing the density\u2192accuracy relationship</li> <li>TwoStageModel combines density modeling with spatial residuals</li> </ul> </li> <li><p>TwoStageModel Advantage: Decomposes the problem into:</p> <ul> <li>Stage 1: Global density effect (monotonic relationship)</li> <li>Stage 2: Location-specific residuals (spatial patterns)</li> </ul> </li> </ol>"},{"location":"examples/Scientific_Visualization/","title":"Scientific Visualization","text":"In\u00a0[1]: Copied! <pre>%load_ext autoreload\n%autoreload 2\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\nimport sys\nfrom pathlib import Path\n\n# Add geoequity to path\nGEOEQUITY_ROOT = Path(\"..\").resolve()\nif str(GEOEQUITY_ROOT) not in sys.path:\n    sys.path.insert(0, str(GEOEQUITY_ROOT))\n\nfrom two_stage import TwoStageModel\nfrom two_stage.model import find_bins_intervals\nfrom two_stage.visualization import plot_predicted_accuracy_map\nfrom visualization import plot_accuracy_map, plot_accuracy_comparison\nfrom data import calculate_density, split_test_train\nfrom data.feature import simple_feature_engineering\n\nprint(\"Ready!\")\n</pre> %load_ext autoreload %autoreload 2  import numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn.linear_model import LinearRegression from sklearn.metrics import r2_score import sys from pathlib import Path  # Add geoequity to path GEOEQUITY_ROOT = Path(\"..\").resolve() if str(GEOEQUITY_ROOT) not in sys.path:     sys.path.insert(0, str(GEOEQUITY_ROOT))  from two_stage import TwoStageModel from two_stage.model import find_bins_intervals from two_stage.visualization import plot_predicted_accuracy_map from visualization import plot_accuracy_map, plot_accuracy_comparison from data import calculate_density, split_test_train from data.feature import simple_feature_engineering  print(\"Ready!\")  <pre>Ready!\n</pre> In\u00a0[2]: Copied! <pre># Load data\ndf = pd.read_pickle('data/df_example.pkl')\ndf['time'] = pd.to_datetime(df['time'])\ndf = df.sample(n=min(50000, len(df)), random_state=42).copy()\ndf['sufficiency'] = len(df)\n\n# Prepare\nTARGET_COL, MODEL_NAME = 'Ozone', 'linear'\ndf = calculate_density(df, radius=500)\n_, _, train_idx, test_idx, df = split_test_train(df, split=0.2, flag='Site', seed=42, verbose=0)\n\n# Train model\nFEATURE_COLS = [c for c in ['TROPOMI_ozone', 'OMI_ozone', 'tco3', 'blh', 't2m'] if c in df.columns]\nX, y, _ = simple_feature_engineering(df, FEATURE_COLS, TARGET_COL, standardize=True, verbose=False)\nmodel = LinearRegression()\nmodel.fit(X.loc[train_idx], y.loc[train_idx])\ndf[f'predicted_{MODEL_NAME}'] = model.predict(X)\ndf['observed'] = df[TARGET_COL]\n\nstations = df.drop_duplicates(subset=['longitude', 'latitude'])\nsufficiency = df['sufficiency'].iloc[0]\nprint(f\"Data: {len(df):,} samples, {len(stations)} stations\")\n</pre> # Load data df = pd.read_pickle('data/df_example.pkl') df['time'] = pd.to_datetime(df['time']) df = df.sample(n=min(50000, len(df)), random_state=42).copy() df['sufficiency'] = len(df)  # Prepare TARGET_COL, MODEL_NAME = 'Ozone', 'linear' df = calculate_density(df, radius=500) _, _, train_idx, test_idx, df = split_test_train(df, split=0.2, flag='Site', seed=42, verbose=0)  # Train model FEATURE_COLS = [c for c in ['TROPOMI_ozone', 'OMI_ozone', 'tco3', 'blh', 't2m'] if c in df.columns] X, y, _ = simple_feature_engineering(df, FEATURE_COLS, TARGET_COL, standardize=True, verbose=False) model = LinearRegression() model.fit(X.loc[train_idx], y.loc[train_idx]) df[f'predicted_{MODEL_NAME}'] = model.predict(X) df['observed'] = df[TARGET_COL]  stations = df.drop_duplicates(subset=['longitude', 'latitude']) sufficiency = df['sufficiency'].iloc[0] print(f\"Data: {len(df):,} samples, {len(stations)} stations\")  <pre>Calculating density (r=500km):   0%|          | 0/1301 [00:00&lt;?, ?it/s]</pre> <pre>Data: 50,000 samples, 1301 stations\n</pre> In\u00a0[8]: Copied! <pre># Fit TwoStageModel\nbins_intervals = find_bins_intervals(df, density_bins=7)\nts_model = TwoStageModel(spline=7, lam=0.5, resolution=[30, 30])\nts_model.fit(df.loc[test_idx], MODEL_NAME, bins_intervals, split_by='grid')\n\n# Plot: Density + Predicted Accuracy\nfig, axes = plot_predicted_accuracy_map(\n    ts_model, \n    stations['longitude'].values, \n    stations['latitude'].values, \n    sufficiency,\n    lon_range=(-10, 35), \n    lat_range=(35, 70), \n    grid_size=30,\n   # accuracy_range=(0, 1)\n)\nplt.suptitle('TwoStageModel: Density \u2192 Accuracy', y=1.02)\nplt.show()\n</pre> # Fit TwoStageModel bins_intervals = find_bins_intervals(df, density_bins=7) ts_model = TwoStageModel(spline=7, lam=0.5, resolution=[30, 30]) ts_model.fit(df.loc[test_idx], MODEL_NAME, bins_intervals, split_by='grid')  # Plot: Density + Predicted Accuracy fig, axes = plot_predicted_accuracy_map(     ts_model,      stations['longitude'].values,      stations['latitude'].values,      sufficiency,     lon_range=(-10, 35),      lat_range=(35, 70),      grid_size=30,    # accuracy_range=(0, 1) ) plt.suptitle('TwoStageModel: Density \u2192 Accuracy', y=1.02) plt.show() <p>Note on Negative R\u00b2 Values:</p> <p>R\u00b2 can be negative when the model performs worse than predicting the mean. This happens when:</p> <ul> <li>Sparse regions have insufficient data for reliable predictions</li> <li>The model has poor generalization in certain areas</li> </ul> <p>If negative R\u00b2 values appear frequently, consider using alternative metrics:</p> <ul> <li>MSE/RMSE: Always positive, easier to interpret</li> <li>MAE: More robust to outliers</li> <li>Correlation (r): Ranges from -1 to 1, captures linear relationship</li> </ul> In\u00a0[7]: Copied! <pre># Compare all modes side by side (including two_stage)\nfig, axes = plot_accuracy_comparison(\n    df.loc[test_idx],\n    model_name=MODEL_NAME,\n    modes=['observation', 'interpolation', 'average_15x20', 'spatial_model', 'two_stage'],\n    accuracy_range=(0, 1),\n    lon_range=(-10, 35),\n    lat_range=(35, 70),\n    # Required for 'two_stage' mode:\n    ts_model=ts_model,\n    station_lons=stations['longitude'].values,\n    station_lats=stations['latitude'].values\n)\nplt.suptitle(f'{MODEL_NAME} - Visualization Modes', y=1.02)\nplt.show()\n</pre> # Compare all modes side by side (including two_stage) fig, axes = plot_accuracy_comparison(     df.loc[test_idx],     model_name=MODEL_NAME,     modes=['observation', 'interpolation', 'average_15x20', 'spatial_model', 'two_stage'],     accuracy_range=(0, 1),     lon_range=(-10, 35),     lat_range=(35, 70),     # Required for 'two_stage' mode:     ts_model=ts_model,     station_lons=stations['longitude'].values,     station_lats=stations['latitude'].values ) plt.suptitle(f'{MODEL_NAME} - Visualization Modes', y=1.02) plt.show()  <pre>&lt;Figure size 640x480 with 0 Axes&gt;</pre>"},{"location":"examples/Scientific_Visualization/#scientific-visualization","title":"Scientific Visualization\u00b6","text":"<p>Visualize spatial accuracy patterns.</p>"},{"location":"examples/Scientific_Visualization/#contents","title":"Contents\u00b6","text":"<ol> <li>Setup - Load data and train models</li> <li>TwoStageModel Prediction - Density + Accuracy map</li> <li>Other Modes - Observation, Interpolation, Average, SVM</li> </ol>"},{"location":"examples/Scientific_Visualization/#1-setup","title":"1. Setup\u00b6","text":""},{"location":"examples/Scientific_Visualization/#2-twostagemodel-prediction","title":"2. TwoStageModel Prediction\u00b6","text":""},{"location":"examples/Scientific_Visualization/#3-other-visualization-modes","title":"3. Other Visualization Modes\u00b6","text":"Mode Description <code>observation</code> Per-station R\u00b2 scatter <code>interpolation</code> IDW interpolation <code>average_NxM</code> Grid-averaged R\u00b2 <code>spatial_model</code> SVM regression <code>two_stage</code> TwoStageModel prediction"},{"location":"examples/Scientific_Visualization/#summary","title":"Summary\u00b6","text":"Function Description <code>plot_predicted_accuracy_map</code> TwoStageModel prediction (density + accuracy) <code>plot_accuracy_comparison</code> Compare multiple modes side by side <code>plot_accuracy_map</code> Single mode visualization"},{"location":"examples/Standard_workflow_geoequity/","title":"TwoStageModel: Spatial Equity Diagnostic","text":"In\u00a0[89]: Copied! <pre>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\nimport sys\nfrom pathlib import Path\n%load_ext autoreload\n%autoreload 2\n\n# Add geoequity to path (for running without pip install)\n# This allows running directly from source code\nGEOEQUITY_ROOT = Path(\"..\").resolve()\nif str(GEOEQUITY_ROOT) not in sys.path:\n    sys.path.insert(0, str(GEOEQUITY_ROOT))\n\n# Import geoequity modules\nfrom two_stage import TwoStageModel\nfrom data import split_test_train, calculate_density, simple_feature_engineering\n\nprint(\"geoequity imported successfully!\")\nprint(f\"Source path: {GEOEQUITY_ROOT}\")\n</pre> import numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn.linear_model import LinearRegression from sklearn.metrics import r2_score import sys from pathlib import Path %load_ext autoreload %autoreload 2  # Add geoequity to path (for running without pip install) # This allows running directly from source code GEOEQUITY_ROOT = Path(\"..\").resolve() if str(GEOEQUITY_ROOT) not in sys.path:     sys.path.insert(0, str(GEOEQUITY_ROOT))  # Import geoequity modules from two_stage import TwoStageModel from data import split_test_train, calculate_density, simple_feature_engineering  print(\"geoequity imported successfully!\") print(f\"Source path: {GEOEQUITY_ROOT}\")  <pre>The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\ngeoequity imported successfully!\nSource path: D:\\OneDrive\\Code\\Ozone_Reconstruction\\Submission_Code\\geoequity\n</pre> In\u00a0[90]: Copied! <pre># ============================================================\n# \ud83c\udfaf CONFIGURATION - Modify these settings for your dataset\n# ============================================================\n\n# Experiment mode\nEXPERIMENT_MODE = 'single'  # Options: 'single' or 'multi'\n# - 'single': Use full dataset (one sufficiency level)\n# - 'multi':  Create 3 sufficiency levels: 5000, 20000, 100000\n\n# Data path\nDATA_PATH = 'data/df_example.pkl'  # Replace with your data file\n\n# Target column (what you want to predict)\nTARGET_COL = 'Ozone'\n\n# Feature columns (predictors for baseline model)\nFEATURE_COLS = ['time','TROPOMI_ozone', 'OMI_ozone', 'tco3', 'blh', 't2m', 'sp', 'WS10', \n 'population', 'no2', 'DSR',\"strd\", \"r_1000\", \"lai_hv\", \"pev\", \"ssro\",\"t_975\",\"t_925\",\"tp\",\"tsn\",\"stl1\",'latitude','longitude']\n\n# Prediction column name (will be created by baseline model)\nPRED_COL = 'predicted_linear'\n</pre>   # ============================================================ # \ud83c\udfaf CONFIGURATION - Modify these settings for your dataset # ============================================================  # Experiment mode EXPERIMENT_MODE = 'single'  # Options: 'single' or 'multi' # - 'single': Use full dataset (one sufficiency level) # - 'multi':  Create 3 sufficiency levels: 5000, 20000, 100000  # Data path DATA_PATH = 'data/df_example.pkl'  # Replace with your data file  # Target column (what you want to predict) TARGET_COL = 'Ozone'  # Feature columns (predictors for baseline model) FEATURE_COLS = ['time','TROPOMI_ozone', 'OMI_ozone', 'tco3', 'blh', 't2m', 'sp', 'WS10',   'population', 'no2', 'DSR',\"strd\", \"r_1000\", \"lai_hv\", \"pev\", \"ssro\",\"t_975\",\"t_925\",\"tp\",\"tsn\",\"stl1\",'latitude','longitude']  # Prediction column name (will be created by baseline model) PRED_COL = 'predicted_linear'  In\u00a0[91]: Copied! <pre># ============================================================\n# Load data\n# ============================================================\ndf = pd.read_pickle(DATA_PATH)\ndf[\"time\"]=pd.to_datetime(df[\"time\"])\nfor col in df.select_dtypes(include=[\"float64\", \"int64\"]).columns:\n    df[col] = df[col].astype(\"float32\")\n    \n# Convert to float32 for memory efficiency\nfor col in df.select_dtypes(include=['float64']).columns:\n    df[col] = df[col].astype('float32')\n\n# Validate required columns\nrequired_cols = ['longitude', 'latitude', 'time', TARGET_COL]\nmissing = [c for c in required_cols if c not in df.columns]\nif missing:\n    raise ValueError(f\"Missing required columns: {missing}\")\n\n# Validate feature columns\navailable_features = [c for c in FEATURE_COLS if c in df.columns]\nif len(available_features) &lt; len(FEATURE_COLS):\n    print(f\"\u26a0\ufe0f Some features not found, using: {available_features}\")\n    FEATURE_COLS = available_features\n\nn_locations = df.groupby(['longitude', 'latitude']).ngroup().nunique()\nprint(f\"Loaded {len(df):,} samples from {n_locations:,} unique locations\")\nprint(f\"Time range: {df['time'].min()} to {df['time'].max()}\")\nprint(f\"\\nTarget: {TARGET_COL}\")\nprint(f\"Features: {FEATURE_COLS}\")\ndf.head()\n</pre>  # ============================================================ # Load data # ============================================================ df = pd.read_pickle(DATA_PATH) df[\"time\"]=pd.to_datetime(df[\"time\"]) for col in df.select_dtypes(include=[\"float64\", \"int64\"]).columns:     df[col] = df[col].astype(\"float32\")      # Convert to float32 for memory efficiency for col in df.select_dtypes(include=['float64']).columns:     df[col] = df[col].astype('float32')  # Validate required columns required_cols = ['longitude', 'latitude', 'time', TARGET_COL] missing = [c for c in required_cols if c not in df.columns] if missing:     raise ValueError(f\"Missing required columns: {missing}\")  # Validate feature columns available_features = [c for c in FEATURE_COLS if c in df.columns] if len(available_features) &lt; len(FEATURE_COLS):     print(f\"\u26a0\ufe0f Some features not found, using: {available_features}\")     FEATURE_COLS = available_features  n_locations = df.groupby(['longitude', 'latitude']).ngroup().nunique() print(f\"Loaded {len(df):,} samples from {n_locations:,} unique locations\") print(f\"Time range: {df['time'].min()} to {df['time'].max()}\") print(f\"\\nTarget: {TARGET_COL}\") print(f\"Features: {FEATURE_COLS}\") df.head() <pre>Loaded 3,520,452 samples from 1,302 unique locations\nTime range: 2019-06-01 00:00:00 to 2019-09-29 23:00:00\n\nTarget: Ozone\nFeatures: ['time', 'TROPOMI_ozone', 'OMI_ozone', 'tco3', 'blh', 't2m', 'sp', 'WS10', 'population', 'no2', 'DSR', 'strd', 'r_1000', 'lai_hv', 'pev', 'ssro', 't_975', 't_925', 'tp', 'tsn', 'stl1', 'latitude', 'longitude']\n</pre> Out[91]: time r_1000 sp stl1 blh t_975 population tsn t2m DSR ... tp no2 strd OMI_ozone Unnamed: 0 site_number sampling_height Ozone latitude longitude 4235938 2019-06-01 00:00:00 40.118874 92562.484375 294.035126 12.997529 301.286194 0.0 273.159912 294.071289 323.755768 ... 0.000003 0.000012 30846750.0 0.000007 3521.0 1.0 2.75 56.968533 35.0 33.0 4235939 2019-06-01 01:00:00 40.820999 92550.921875 293.471710 11.205620 301.448181 0.0 273.159668 293.956604 323.060455 ... 0.000000 0.000013 1177312.0 0.000007 3522.0 1.0 2.75 60.438416 35.0 33.0 4235940 2019-06-01 02:00:00 39.497925 92548.421875 292.972778 13.652981 301.544983 0.0 273.159668 293.848602 322.365143 ... 0.000000 0.000013 2347693.5 0.000007 3523.0 1.0 2.75 59.958427 35.0 33.0 4235941 2019-06-01 03:00:00 37.452812 92548.109375 292.592804 19.237371 301.245117 0.0 273.161133 294.628082 321.669861 ... 0.000000 0.000013 3514501.0 0.000006 3524.0 1.0 2.75 56.148556 35.0 33.0 4235942 2019-06-01 04:00:00 36.479145 92570.617188 293.640198 35.947742 300.535797 0.0 273.159668 295.702423 320.974548 ... 0.000000 0.000014 4685279.5 0.000006 3525.0 1.0 2.75 56.758514 35.0 33.0 <p>5 rows \u00d7 29 columns</p> In\u00a0[92]: Copied! <pre># ============================================================\n# Create sufficiency levels based on experiment mode\n# ============================================================\nif EXPERIMENT_MODE == 'multi':\n    # Multi-sufficiency: sample at 3 different sizes\n    SUFFICIENCY_LEVELS = [5000, 20000, 100000]\n    \n    np.random.seed(42)\n    df_list = []\n    for suff in SUFFICIENCY_LEVELS:\n        if len(df) &gt;= suff:\n            df_sampled = df.sample(n=suff, random_state=42).copy()\n        else:\n            df_sampled = df.copy()\n        df_sampled['sufficiency'] = suff\n        df_list.append(df_sampled)\n    \n    df = pd.concat(df_list, ignore_index=True)\n    print(f\"Multi-sufficiency mode: {SUFFICIENCY_LEVELS}\")\n    print(f\"Total samples: {len(df):,}\")\n    print(f\"\\nSamples per sufficiency level:\")\n    print(df['sufficiency'].value_counts().sort_index())\n    \nelse:\n    # Single sufficiency: use 100k (or as many samples as available if less)\n    n_suff = min(100000, len(df))\n    if len(df) &gt; n_suff:\n        df = df.sample(n=n_suff, random_state=42).copy()\n    df['sufficiency'] = 100000\n    print(f\"Single sufficiency mode: using {len(df):,} samples (target 100,000)\")\n</pre> # ============================================================ # Create sufficiency levels based on experiment mode # ============================================================ if EXPERIMENT_MODE == 'multi':     # Multi-sufficiency: sample at 3 different sizes     SUFFICIENCY_LEVELS = [5000, 20000, 100000]          np.random.seed(42)     df_list = []     for suff in SUFFICIENCY_LEVELS:         if len(df) &gt;= suff:             df_sampled = df.sample(n=suff, random_state=42).copy()         else:             df_sampled = df.copy()         df_sampled['sufficiency'] = suff         df_list.append(df_sampled)          df = pd.concat(df_list, ignore_index=True)     print(f\"Multi-sufficiency mode: {SUFFICIENCY_LEVELS}\")     print(f\"Total samples: {len(df):,}\")     print(f\"\\nSamples per sufficiency level:\")     print(df['sufficiency'].value_counts().sort_index())      else:     # Single sufficiency: use 100k (or as many samples as available if less)     n_suff = min(100000, len(df))     if len(df) &gt; n_suff:         df = df.sample(n=n_suff, random_state=42).copy()     df['sufficiency'] = 100000     print(f\"Single sufficiency mode: using {len(df):,} samples (target 100,000)\")  <pre>Single sufficiency mode: using 100,000 samples (target 100,000)\n</pre> In\u00a0[93]: Copied! <pre># Calculate density with 500km radius\n# In real case, you should calculate the density based on training stations\ndf = calculate_density(df, radius=500)\n\nprint(f\"Density range: {df['density'].min():.2e} - {df['density'].max():.2e}\")\n\n# Visualize density distribution\nfig, ax = plt.subplots(figsize=(5, 3))\nscatter = ax.scatter(df['longitude'], df['latitude'], c=df['density'], \n                     cmap='viridis', s=3, alpha=0.5)\nplt.colorbar(scatter, label='Data Density (stations/km\u00b2)')\nax.set_xlabel('Longitude')\nax.set_ylabel('Latitude')\nax.set_title('Observation Station Density Distribution')\nplt.tight_layout()\nplt.show()\n</pre> # Calculate density with 500km radius # In real case, you should calculate the density based on training stations df = calculate_density(df, radius=500)  print(f\"Density range: {df['density'].min():.2e} - {df['density'].max():.2e}\")  # Visualize density distribution fig, ax = plt.subplots(figsize=(5, 3)) scatter = ax.scatter(df['longitude'], df['latitude'], c=df['density'],                       cmap='viridis', s=3, alpha=0.5) plt.colorbar(scatter, label='Data Density (stations/km\u00b2)') ax.set_xlabel('Longitude') ax.set_ylabel('Latitude') ax.set_title('Observation Station Density Distribution') plt.tight_layout() plt.show()  <pre>Calculating density (r=500km):   0%|          | 0/1302 [00:00&lt;?, ?it/s]</pre> <pre>Density range: 1.81e-06 - 2.11e-04\n</pre> In\u00a0[94]: Copied! <pre># Site-wise split using split_test_train\n# flag='Site' ensures same station's observations stay in same set\n\nif EXPERIMENT_MODE == 'multi':\n    # Multi: split each sufficiency level separately\n    split_results = {}  # {sufficiency: (train_idx, test_idx)}\n    for suff in df['sufficiency'].unique():\n        df_suff = df[df['sufficiency'] == suff].copy()\n        _, _, train_idx_s, test_idx_s, _ = split_test_train(\n            df_suff, split=0.2, flag='Site', seed=42, verbose=0\n        )\n        split_results[suff] = (train_idx_s, test_idx_s)\n        print(f\"Sufficiency {suff:,}: Train {len(train_idx_s)}, Test {len(test_idx_s)}\")\nelse:\n    # Single: original logic\n    train_sites, test_sites, train_idx, test_idx, df = split_test_train(\n        df, split=0.2, flag='Site', seed=42, verbose=1\n    )\n    print(f\"\\nTrain: {len(train_idx)} samples, Test: {len(test_idx)} samples\")\n</pre> # Site-wise split using split_test_train # flag='Site' ensures same station's observations stay in same set  if EXPERIMENT_MODE == 'multi':     # Multi: split each sufficiency level separately     split_results = {}  # {sufficiency: (train_idx, test_idx)}     for suff in df['sufficiency'].unique():         df_suff = df[df['sufficiency'] == suff].copy()         _, _, train_idx_s, test_idx_s, _ = split_test_train(             df_suff, split=0.2, flag='Site', seed=42, verbose=0         )         split_results[suff] = (train_idx_s, test_idx_s)         print(f\"Sufficiency {suff:,}: Train {len(train_idx_s)}, Test {len(test_idx_s)}\") else:     # Single: original logic     train_sites, test_sites, train_idx, test_idx, df = split_test_train(         df, split=0.2, flag='Site', seed=42, verbose=1     )     print(f\"\\nTrain: {len(train_idx)} samples, Test: {len(test_idx)} samples\")  <pre>Using flag: Site\nSelected Site Count: 260, (19.97%)\nSelected DataRow Count: 19765, (19.77%)\nTraining Site Count: 1042, (80.03%)\nTraining DataRow Count: 80235, (80.23%)\n\nTrain: 80235 samples, Test: 19765 samples\n</pre> In\u00a0[95]: Copied! <pre># ============================================================\n# Feature Engineering &amp; Train - per sufficiency level if multi\n# ============================================================\ndf[PRED_COL] = np.nan  # Initialize prediction column\n\nif EXPERIMENT_MODE == 'multi':\n    # Multi: train each sufficiency level separately\n    for suff in df['sufficiency'].unique():\n        print(f\"\\n{'='*50}\")\n        print(f\"Training for Sufficiency = {suff:,}\")\n        print(f\"{'='*50}\")\n        \n        # Get data for this sufficiency\n        df_suff = df[df['sufficiency'] == suff]\n        train_idx_s, test_idx_s = split_results[suff]\n        \n        # Feature engineering\n        X_suff, y_suff, _ = simple_feature_engineering(\n            df_suff, FEATURE_COLS, TARGET_COL,\n            add_spatial_harmonics=True, add_temporal_harmonics=True,\n            standardize=True, verbose=False\n        )\n        \n        X_train = X_suff.loc[train_idx_s]\n        y_train = y_suff.loc[train_idx_s]\n        X_test = X_suff.loc[test_idx_s]\n        y_test = y_suff.loc[test_idx_s]\n        \n        # Train model\n        model = LinearRegression()\n        model.fit(X_train, y_train)\n        \n        # Add predictions back to df\n        df.loc[train_idx_s, PRED_COL] = model.predict(X_train)\n        df.loc[test_idx_s, PRED_COL] = model.predict(X_test)\n        \n        # Evaluate\n        test_r2 = r2_score(y_test, df.loc[test_idx_s, PRED_COL])\n        print(f\"Test R\u00b2: {test_r2:.4f}\")\n    \n    # Store all test indices for TwoStageModel\n    all_test_idx = []\n    for suff in df['sufficiency'].unique():\n        all_test_idx.extend(split_results[suff][1])\n    test_idx = all_test_idx\n\nelse:\n    # Single: original logic\n    X, y, FINAL_FEATURES = simple_feature_engineering(\n        df, FEATURE_COLS, TARGET_COL,\n        add_spatial_harmonics=True, add_temporal_harmonics=True, standardize=True\n    )\n    \n    X_train = X.loc[train_idx]\n    y_train = y.loc[train_idx]\n    X_test = X.loc[test_idx]\n    y_test = y.loc[test_idx]\n    \n    print(f\"\\nTrain shape: {X_train.shape}, Test shape: {X_test.shape}\")\n    \n    model = LinearRegression()\n    model.fit(X_train, y_train)\n    \n    df.loc[train_idx, PRED_COL] = model.predict(X_train)\n    df.loc[test_idx, PRED_COL] = model.predict(X_test)\n    \n    train_r2 = r2_score(y_train, df.loc[train_idx, PRED_COL])\n    test_r2 = r2_score(y_test, df.loc[test_idx, PRED_COL])\n    \n    print(f\"\\n{'='*50}\")\n    print(f\"Model Evaluation\")\n    print(f\"{'='*50}\")\n    print(f\"Train R\u00b2: {train_r2:.4f}\")\n    print(f\"Test R\u00b2:  {test_r2:.4f}\")\n</pre> # ============================================================ # Feature Engineering &amp; Train - per sufficiency level if multi # ============================================================ df[PRED_COL] = np.nan  # Initialize prediction column  if EXPERIMENT_MODE == 'multi':     # Multi: train each sufficiency level separately     for suff in df['sufficiency'].unique():         print(f\"\\n{'='*50}\")         print(f\"Training for Sufficiency = {suff:,}\")         print(f\"{'='*50}\")                  # Get data for this sufficiency         df_suff = df[df['sufficiency'] == suff]         train_idx_s, test_idx_s = split_results[suff]                  # Feature engineering         X_suff, y_suff, _ = simple_feature_engineering(             df_suff, FEATURE_COLS, TARGET_COL,             add_spatial_harmonics=True, add_temporal_harmonics=True,             standardize=True, verbose=False         )                  X_train = X_suff.loc[train_idx_s]         y_train = y_suff.loc[train_idx_s]         X_test = X_suff.loc[test_idx_s]         y_test = y_suff.loc[test_idx_s]                  # Train model         model = LinearRegression()         model.fit(X_train, y_train)                  # Add predictions back to df         df.loc[train_idx_s, PRED_COL] = model.predict(X_train)         df.loc[test_idx_s, PRED_COL] = model.predict(X_test)                  # Evaluate         test_r2 = r2_score(y_test, df.loc[test_idx_s, PRED_COL])         print(f\"Test R\u00b2: {test_r2:.4f}\")          # Store all test indices for TwoStageModel     all_test_idx = []     for suff in df['sufficiency'].unique():         all_test_idx.extend(split_results[suff][1])     test_idx = all_test_idx  else:     # Single: original logic     X, y, FINAL_FEATURES = simple_feature_engineering(         df, FEATURE_COLS, TARGET_COL,         add_spatial_harmonics=True, add_temporal_harmonics=True, standardize=True     )          X_train = X.loc[train_idx]     y_train = y.loc[train_idx]     X_test = X.loc[test_idx]     y_test = y.loc[test_idx]          print(f\"\\nTrain shape: {X_train.shape}, Test shape: {X_test.shape}\")          model = LinearRegression()     model.fit(X_train, y_train)          df.loc[train_idx, PRED_COL] = model.predict(X_train)     df.loc[test_idx, PRED_COL] = model.predict(X_test)          train_r2 = r2_score(y_train, df.loc[train_idx, PRED_COL])     test_r2 = r2_score(y_test, df.loc[test_idx, PRED_COL])          print(f\"\\n{'='*50}\")     print(f\"Model Evaluation\")     print(f\"{'='*50}\")     print(f\"Train R\u00b2: {train_r2:.4f}\")     print(f\"Test R\u00b2:  {test_r2:.4f}\")  <pre>Feature Engineering Pipeline: fill NA \u2192 temporal harmonics \u2192 spatial harmonics \u2192 standardize\n  \u2713 Added temporal harmonics: T1, T2, T3, month, hour\n  \u2713 Added spatial harmonics: S1, S2, S3 (replaced lon/lat)\n  \u2713 Standardized all features\n\nFinal features (28): ['TROPOMI_ozone', 'OMI_ozone', 'tco3', 'blh', 't2m']...\n\nTrain shape: (80235, 28), Test shape: (19765, 28)\n\n==================================================\nModel Evaluation\n==================================================\nTrain R\u00b2: 0.4953\nTest R\u00b2:  0.4897\n</pre> In\u00a0[96]: Copied! <pre># ============================================================\n# Prepare data for TwoStageModel\n# ============================================================\nfrom two_stage.model import find_bins_intervals\n\n# Add 'observed' column (required by TwoStageModel)\ndf['observed'] = df[TARGET_COL]\n\n# Extract model name from prediction column (e.g., 'predicted_linear' -&gt; 'linear')\nMODEL_NAME = PRED_COL.replace('predicted_', '')\n\n# Create bins intervals for density and sufficiency\nbins_intervals = find_bins_intervals(df, density_bins=7)\nprint(f\"Created bins for {len(bins_intervals[1])} sufficiency levels\")\n\n# ============================================================\n# Initialize and fit TwoStageModel\n# ============================================================\nts_model = TwoStageModel(\n    spline=7,           # GAM spline knots\n    lam=0.5,            # GAM regularization\n    resolution=[30, 30] # Spatial aggregation grid\n)\n\n# Fit model using test data to analyze accuracy patterns\n# This learns how accuracy varies with density and location\n# In real case, you should use the density based on training stations\nts_model.fit(\n    df_train_raw=df.loc[test_idx],\n    model_name=MODEL_NAME,\n    bins_intervals=bins_intervals,\n    split_by='grid'\n)\n\nprint(f\"\\n{'='*50}\")\nprint(f\"TwoStageModel Training Complete!\")\nprint(f\"{'='*50}\")\nprint(f\"Model: {PRED_COL} \u2192 {TARGET_COL}\")\nprint(f\"Stage 1 R\u00b2 (GAM on density): {ts_model.stage1_score:.4f}\")\nprint(f\"Stage 2 R\u00b2 (GAM+SVM spatial): {ts_model.stage2_score:.4f}\")\nprint(f\"Mode: {'Single' if ts_model.single_sufficiency else 'Multi'} Sufficiency\")\n</pre> # ============================================================ # Prepare data for TwoStageModel # ============================================================ from two_stage.model import find_bins_intervals  # Add 'observed' column (required by TwoStageModel) df['observed'] = df[TARGET_COL]  # Extract model name from prediction column (e.g., 'predicted_linear' -&gt; 'linear') MODEL_NAME = PRED_COL.replace('predicted_', '')  # Create bins intervals for density and sufficiency bins_intervals = find_bins_intervals(df, density_bins=7) print(f\"Created bins for {len(bins_intervals[1])} sufficiency levels\")  # ============================================================ # Initialize and fit TwoStageModel # ============================================================ ts_model = TwoStageModel(     spline=7,           # GAM spline knots     lam=0.5,            # GAM regularization     resolution=[30, 30] # Spatial aggregation grid )  # Fit model using test data to analyze accuracy patterns # This learns how accuracy varies with density and location # In real case, you should use the density based on training stations ts_model.fit(     df_train_raw=df.loc[test_idx],     model_name=MODEL_NAME,     bins_intervals=bins_intervals,     split_by='grid' )  print(f\"\\n{'='*50}\") print(f\"TwoStageModel Training Complete!\") print(f\"{'='*50}\") print(f\"Model: {PRED_COL} \u2192 {TARGET_COL}\") print(f\"Stage 1 R\u00b2 (GAM on density): {ts_model.stage1_score:.4f}\") print(f\"Stage 2 R\u00b2 (GAM+SVM spatial): {ts_model.stage2_score:.4f}\") print(f\"Mode: {'Single' if ts_model.single_sufficiency else 'Multi'} Sufficiency\")  <pre>Created bins for 1 sufficiency levels\n\n==================================================\nTwoStageModel Training Complete!\n==================================================\nModel: predicted_linear \u2192 Ozone\nStage 1 R\u00b2 (GAM on density): 0.9872\nStage 2 R\u00b2 (GAM+SVM spatial): 0.6429\nMode: Single Sufficiency\n</pre> In\u00a0[97]: Copied! <pre># Your network stations and sample size\nstations = df.drop_duplicates(subset=['longitude', 'latitude'])\nsufficiency = df['sufficiency'].unique()[0]\n\n# Import visualization tools\nfrom two_stage import plot_predicted_accuracy_map, predict_at_locations\n\n# Example 1: Predict at specific locations\nr2_dense, _ = predict_at_locations(ts_model, 5.0, 50.0, stations['longitude'], stations['latitude'], sufficiency)\nr2_sparse, _ = predict_at_locations(ts_model, 30.0, 55.0, stations['longitude'], stations['latitude'], sufficiency)\nprint(f\"Dense area (5\u00b0E, 50\u00b0N):  R\u00b2 = {r2_dense[0]:.4f}\")\nprint(f\"Sparse area (30\u00b0E, 55\u00b0N): R\u00b2 = {r2_sparse[0]:.4f}\")\nprint(f\"Equity Gap: {float(r2_dense[0] - r2_sparse[0]):.4f}\")\n\n# Example 2: Plot predicted accuracy map\nfig, axes = plot_predicted_accuracy_map(\n    ts_model, stations['longitude'], stations['latitude'], sufficiency,\n    lon_range=(-10, 35), lat_range=(35, 70), grid_size=30\n)\nplt.show()\n</pre> # Your network stations and sample size stations = df.drop_duplicates(subset=['longitude', 'latitude']) sufficiency = df['sufficiency'].unique()[0]  # Import visualization tools from two_stage import plot_predicted_accuracy_map, predict_at_locations  # Example 1: Predict at specific locations r2_dense, _ = predict_at_locations(ts_model, 5.0, 50.0, stations['longitude'], stations['latitude'], sufficiency) r2_sparse, _ = predict_at_locations(ts_model, 30.0, 55.0, stations['longitude'], stations['latitude'], sufficiency) print(f\"Dense area (5\u00b0E, 50\u00b0N):  R\u00b2 = {r2_dense[0]:.4f}\") print(f\"Sparse area (30\u00b0E, 55\u00b0N): R\u00b2 = {r2_sparse[0]:.4f}\") print(f\"Equity Gap: {float(r2_dense[0] - r2_sparse[0]):.4f}\")  # Example 2: Plot predicted accuracy map fig, axes = plot_predicted_accuracy_map(     ts_model, stations['longitude'], stations['latitude'], sufficiency,     lon_range=(-10, 35), lat_range=(35, 70), grid_size=30 ) plt.show()  <pre>Dense area (5\u00b0E, 50\u00b0N):  R\u00b2 = 0.6148\nSparse area (30\u00b0E, 55\u00b0N): R\u00b2 = 0.5680\nEquity Gap: 0.0468\n</pre> In\u00a0[98]: Copied! <pre># Generate diagnostics\ndiagnosis = ts_model.diagnose(save_dir='diagnostics/', show=True)\n\nprint(\"\\nDiagnostic files saved:\")\nprint(\"  - diagnostics/stage1_gam.png\")\nprint(\"  - diagnostics/stage2_svm.png\")\nprint(\"  - diagnostics/diagnosis.txt\")\n</pre> # Generate diagnostics diagnosis = ts_model.diagnose(save_dir='diagnostics/', show=True)  print(\"\\nDiagnostic files saved:\") print(\"  - diagnostics/stage1_gam.png\") print(\"  - diagnostics/stage2_svm.png\") print(\"  - diagnostics/diagnosis.txt\")  <pre>\u2713 Stage 1 diagram saved to: diagnostics/stage1_gam.png\n</pre> <pre>\u2713 Stage 2 diagram saved to: diagnostics/stage2_svm.png\n</pre> <pre>\u2713 Diagnosis saved to: diagnostics//\n\nDiagnostic files saved:\n  - diagnostics//stage1_gam.png\n  - diagnostics//stage2_svm.png\n  - diagnostics//diagnosis.txt\n\n========== TwoStageModel Diagnosis ==========\n\nMode: Single Sufficiency\n\nStage 1 (Monotonic GAM) - Density/Sampling Effect:\n  - r = 0.9872, MAE = 0.0214\n  - Training samples: 7\n  - Spline knots: 7, Lambda: 0.5\n\nStage 2 (SVM) - Spatial Residual:\n  - r = 0.4986, MAE = 0.1830\n  - Training samples: 152\n  - Spatial features: ['longitude', 'latitude']\n  - Resolution: [30, 30]\n\nTotal (GAM + SVM):\n  - r = 0.6429, MAE = 0.1830\n\nKey Insights:\n  1. Density effect captured in Stage 1 (r=0.987)\n     \u2192 Predicts accuracy variation from sampling density\n  2. Spatial pattern captured in Stage 2 (r=0.499)\n     \u2192 Predicts location-specific residuals\n  3. Prediction ability:\n     \u2192 Unseen sampling density: r=0.987 (Stage 1)\n     \u2192 Unseen locations: r=0.643 (Total)\n\n=============================================\n\n\nDiagnostic files saved:\n  - diagnostics/stage1_gam.png\n  - diagnostics/stage2_svm.png\n  - diagnostics/diagnosis.txt\n</pre>"},{"location":"examples/Standard_workflow_geoequity/#twostagemodel-spatial-equity-diagnostic","title":"TwoStageModel: Spatial Equity Diagnostic\u00b6","text":"<p>This notebook demonstrates the complete workflow for diagnosing spatial accuracy patterns using <code>TwoStageModel</code>.</p>"},{"location":"examples/Standard_workflow_geoequity/#two-experiment-modes","title":"Two Experiment Modes\u00b6","text":"Mode Description Use Case Single Sufficiency One fixed sample size Quick analysis, production models Multi Sufficiency Multiple sample sizes (5K, 20K, 100K) Studying how accuracy changes with data volume"},{"location":"examples/Standard_workflow_geoequity/#overview","title":"Overview\u00b6","text":"<ol> <li>Configuration - Set data path, features, target</li> <li>Sufficiency Levels - Single or multi-sufficiency experiment</li> <li>Data Density - Calculate station density</li> <li>Data Split - Site-wise cross-validation</li> <li>Feature Engineering - Spatial/temporal harmonics (customizable!)</li> <li>Train Model - Baseline ML model</li> <li>TwoStageModel - Fit &amp; diagnose</li> <li>Predict - Accuracy at new locations</li> </ol>"},{"location":"examples/Standard_workflow_geoequity/#1-configuration-load-data","title":"1. Configuration &amp; Load Data\u00b6","text":"<p>Modify the configuration below to use your own dataset!</p>"},{"location":"examples/Standard_workflow_geoequity/#required-columns","title":"Required columns:\u00b6","text":"<ul> <li><code>longitude</code>, <code>latitude</code>: Spatial coordinates (float)</li> <li><code>time</code>: Timestamp (datetime)</li> <li>Target column (e.g., <code>o3</code>): Observed values (float)</li> </ul>"},{"location":"examples/Standard_workflow_geoequity/#configuration-variables","title":"Configuration variables:\u00b6","text":"Variable Description <code>DATA_PATH</code> Path to your pickle file <code>TARGET_COL</code> Column name for target values <code>FEATURE_COLS</code> List of feature column names <code>PRED_COL</code> Name for prediction column <p>The example uses ozone observations from June-September 2019 (3.5M samples).</p>"},{"location":"examples/Standard_workflow_geoequity/#11-create-sufficiency-levels","title":"1.1 Create Sufficiency Levels\u00b6","text":"<p>Single mode: Use full dataset as one sufficiency level (100k).</p> <p>Multi mode: Sample data at 3 sizes (5K, 20K, 100K) to study how accuracy varies with data volume.</p>"},{"location":"examples/Standard_workflow_geoequity/#2-calculate-data-density","title":"2. Calculate Data Density\u00b6","text":"<p>Calculate data density for each location. Higher density = more nearby observation stations.</p>"},{"location":"examples/Standard_workflow_geoequity/#3-split-data-site-wise-cross-validation","title":"3. Split Data (Site-wise Cross-Validation)\u00b6","text":"<p>Use site-wise split to avoid data leakage - same station's observations stay in same set.</p>"},{"location":"examples/Standard_workflow_geoequity/#4-feature-engineering-train-model","title":"4. Feature Engineering &amp; Train Model\u00b6","text":""},{"location":"examples/Standard_workflow_geoequity/#feature-engineering-options","title":"Feature Engineering Options\u00b6","text":"<p>You can use your own feature engineering method! The provided <code>simple_feature_engineering</code> is just an example.</p> Step Description Fill NA Replace missing values with median Temporal Harmonics Add T1, T2, T3 (Fourier features for seasonality) Spatial Harmonics Add S1, S2, S3 (spherical harmonics from lon/lat) Standardize Zero mean, unit variance scaling <pre># Option A: Use provided feature engineering\nX, y, feature_names = simple_feature_engineering(df, FEATURE_COLS, TARGET_COL)\n\n# Option B: Use your own method\nX = your_custom_preprocessing(df[FEATURE_COLS])\ny = df[TARGET_COL]\n</pre>"},{"location":"examples/Standard_workflow_geoequity/#5-fit-twostagemodel","title":"5. Fit TwoStageModel\u00b6","text":"<p>Analyze how accuracy varies with data density and location.</p>"},{"location":"examples/Standard_workflow_geoequity/#6-predict-accuracy-for-new-locations","title":"6. Predict Accuracy for New Locations\u00b6","text":"<p>Use the fitted model to predict expected accuracy (R\u00b2) at any location.</p> <p>Key Points:</p> <ul> <li><code>density</code> must be calculated per-pixel (based on distance to training stations)</li> <li>For multi-sufficiency mode, we use the first sufficiency level as default</li> <li>Predictions use the new API: <code>predict(longitude, latitude, density, sufficiency)</code></li> </ul>"},{"location":"examples/Standard_workflow_geoequity/#7-generate-diagnostic-report","title":"7. Generate Diagnostic Report\u00b6","text":"<p>Generate diagnostic plots and text report for model analysis.</p>"},{"location":"examples/Standard_workflow_geoequity/#summary","title":"Summary\u00b6","text":""},{"location":"examples/Standard_workflow_geoequity/#core-concepts","title":"Core Concepts\u00b6","text":"Term Definition Sufficiency Training sample size (e.g., 5K, 20K, 100K observations) Density Observation station density at a location (stations/km\u00b2 within radius)"},{"location":"examples/Standard_workflow_geoequity/#workflow","title":"Workflow\u00b6","text":"Step Function Description 1 <code>calculate_density()</code> Calculate station density for each location 2 <code>split_test_train()</code> Site-wise split avoiding data leakage 3 <code>TwoStageModel.fit()</code> Learn density\u2192accuracy + spatial patterns 4 <code>TwoStageModel.predict()</code> Predict accuracy at any location 5 <code>TwoStageModel.diagnose()</code> Generate diagnostic plots &amp; report"},{"location":"examples/Standard_workflow_geoequity/#key-insight","title":"Key Insight\u00b6","text":"<p>Machine learning models trained on geospatial data exhibit spatial inequity: prediction accuracy varies systematically across space. This inequity arises from two sources:</p> <ol> <li><p>Sampling Density Effect (Stage 1): Regions with denser observation networks provide more training data, leading to higher local accuracy. The monotonic GAM captures this density\u2192accuracy relationship.</p> </li> <li><p>Spatial Residual Effect (Stage 2): Even after accounting for density, some locations have systematically higher/lower accuracy due to local data quality, terrain complexity, or other spatial factors. The SVM captures these location-specific patterns.</p> </li> </ol> <p>TwoStageModel quantifies and predicts this spatial inequity, enabling:</p> <ul> <li>Fair assessment of model performance across regions</li> <li>Identification of under-served areas needing more observations</li> <li>Uncertainty quantification for downstream applications</li> </ul>"},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#from-pypi","title":"From PyPI","text":"<pre><code>pip install geoequity\n</code></pre>"},{"location":"getting-started/installation/#from-source","title":"From Source","text":"<pre><code>git clone https://github.com/px39n/geoequity.git\ncd geoequity\npip install -e .\n</code></pre>"},{"location":"getting-started/installation/#optional-dependencies","title":"Optional Dependencies","text":"<p>For full functionality including GAM models and geospatial visualization:</p> <pre><code>pip install geoequity[full]\n</code></pre> <p>This installs: - <code>pygam</code> - For Generalized Additive Models - <code>xarray</code> - For gridded data handling - <code>geopandas</code> - For geospatial data operations</p>"},{"location":"getting-started/installation/#requirements","title":"Requirements","text":"<ul> <li>Python &gt;= 3.8</li> <li>numpy &gt;= 1.20</li> <li>pandas &gt;= 1.3</li> <li>scikit-learn &gt;= 1.0</li> <li>matplotlib &gt;= 3.4</li> <li>tqdm &gt;= 4.60</li> </ul>"},{"location":"getting-started/installation/#verify-installation","title":"Verify Installation","text":"<pre><code>import geoequity\nprint(geoequity.__version__)\n</code></pre>"},{"location":"getting-started/quickstart/","title":"Quick Start","text":""},{"location":"getting-started/quickstart/#overview","title":"Overview","text":"<pre><code>Data Preparation \u2192 TwoStageModel.fit() \u2192 Predict &amp; Diagnose\n</code></pre>"},{"location":"getting-started/quickstart/#part-1-data-preparation","title":"Part 1: Data Preparation","text":"<p>Before using GeoEquity, you need a DataFrame with validated ML predictions.</p>"},{"location":"getting-started/quickstart/#required-columns","title":"Required Columns","text":"Column Type Description <code>longitude</code> float Spatial coordinate <code>latitude</code> float Spatial coordinate <code>observed</code> float Ground truth values <code>predicted_{model_name}</code> float Your model's predictions <code>density</code> float Data density at location <code>sufficiency</code> int Training sample size"},{"location":"getting-started/quickstart/#step-1-load-data","title":"Step 1: Load Data","text":"<pre><code>import pandas as pd\n\ndf = pd.read_pickle('your_data.pkl')\ndf['time'] = pd.to_datetime(df['time'])\n\n# Memory optimization\nfor col in df.select_dtypes(include=['float64']).columns:\n    df[col] = df[col].astype('float32')\n</code></pre>"},{"location":"getting-started/quickstart/#step-2-calculate-density","title":"Step 2: Calculate Density","text":"<pre><code>from geoequity.data import calculate_density\n\ndf = calculate_density(df, radius=500)  # 500 km search radius\n</code></pre>"},{"location":"getting-started/quickstart/#step-3-split-data","title":"Step 3: Split Data","text":"<pre><code>from geoequity.data import split_test_train\n\n_, _, train_idx, test_idx, df = split_test_train(\n    df, split=0.2, flag='Site', seed=42\n)\n</code></pre>"},{"location":"getting-started/quickstart/#step-4-train-your-model-add-predictions","title":"Step 4: Train Your Model &amp; Add Predictions","text":"<pre><code>from sklearn.linear_model import LinearRegression\nfrom geoequity.data import simple_feature_engineering\n\n# Feature engineering\nFEATURE_COLS = ['feature1', 'feature2', 'latitude', 'longitude']\nTARGET_COL = 'target'\nX, y, _ = simple_feature_engineering(df, FEATURE_COLS, TARGET_COL)\n\n# Train model\nmodel = LinearRegression()\nmodel.fit(X.loc[train_idx], y.loc[train_idx])\n\n# Add required columns\ndf['predicted_linear'] = model.predict(X)\ndf['observed'] = df[TARGET_COL]\ndf['sufficiency'] = len(train_idx)\n</code></pre>"},{"location":"getting-started/quickstart/#part-2-spatial-equity-analysis","title":"Part 2: Spatial Equity Analysis","text":"<p>With your prepared data, analyze spatial accuracy patterns.</p>"},{"location":"getting-started/quickstart/#fit-twostagemodel","title":"Fit TwoStageModel","text":"<pre><code>from geoequity import TwoStageModel\nfrom geoequity.two_stage.model import find_bins_intervals\n\nbins_intervals = find_bins_intervals(df, density_bins=7)\n\nts = TwoStageModel(spline=7, lam=0.5, resolution=[30, 30])\nts.fit(\n    df_train_raw=df.loc[test_idx],\n    model_name='linear',\n    bins_intervals=bins_intervals\n)\n\nprint(f\"Stage 1 R\u00b2 (density): {ts.stage1_score:.4f}\")\nprint(f\"Stage 2 R\u00b2 (spatial): {ts.stage2_score:.4f}\")\n</code></pre>"},{"location":"getting-started/quickstart/#predict-accuracy","title":"Predict Accuracy","text":"<pre><code>from geoequity.two_stage import predict_at_locations\n\nstations = df.drop_duplicates(subset=['longitude', 'latitude'])\n\nr2, density = predict_at_locations(\n    ts, longitude=5.0, latitude=50.0,\n    station_lons=stations['longitude'],\n    station_lats=stations['latitude'],\n    sufficiency=df['sufficiency'].iloc[0]\n)\nprint(f\"Predicted R\u00b2 at (5\u00b0E, 50\u00b0N): {r2[0]:.4f}\")\n</code></pre>"},{"location":"getting-started/quickstart/#generate-diagnostics","title":"Generate Diagnostics","text":"<pre><code>ts.diagnose(save_dir='diagnostics/', show=True)\n</code></pre> <p>Outputs: - <code>stage1_gam.png</code> - Density \u2192 Accuracy relationship - <code>stage2_svm.png</code> - Spatial residual patterns - <code>diagnosis.txt</code> - Summary statistics</p>"},{"location":"getting-started/quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Two-Stage Model details</li> <li>Model Comparison</li> <li>Visualization options</li> </ul>"},{"location":"guide/comparison/","title":"Model Comparison","text":"<p>Compare different baseline methods for predicting spatial accuracy patterns.</p>"},{"location":"guide/comparison/#evaluation-scenarios","title":"Evaluation Scenarios","text":"Scenario Split Method Question Answered Unseen Spatial spatial How well predict accuracy at new locations? Unseen Sampling sampling How well predict accuracy for new density levels?"},{"location":"guide/comparison/#models-compared","title":"Models Compared","text":"Category Model Description Traditional ML <code>linear</code> Linear Regression <code>svm</code> Support Vector Regression (RBF) <code>lightgbm</code> Gradient Boosting GAM <code>gam_monotonic</code> Monotonic GAM with interaction Interpolation <code>interpolation</code> IDW (Inverse Distance Weighting) Two-Stage <code>two_stage</code> GAM (density) + SVM (spatial residual)"},{"location":"guide/comparison/#usage","title":"Usage","text":"<pre><code>from geoequity.evaluation import eval_baseline_comparison\n\n# Define models to compare\nmodel_list = ['linear', 'lightgbm', 'svm', 'gam_monotonic', 'interpolation', 'two_stage']\n\n# Scenario 1: Unseen Spatial\nreport_spatial = eval_baseline_comparison(\n    df_analysis,\n    model_list=model_list,\n    density_bins=30,\n    split_method='spatial',\n    train_by='grid',\n    evaluate_by='grid',\n    metric='correlation',\n    full_features='Spatial'\n)\n\n# Scenario 2: Unseen Sampling\nreport_sampling = eval_baseline_comparison(\n    df_analysis,\n    model_list=model_list,\n    density_bins=30,\n    split_method='sampling',\n    train_by='grid',\n    evaluate_by='sampling',\n    metric='correlation',\n    full_features='Spatial'\n)\n</code></pre>"},{"location":"guide/comparison/#visualization","title":"Visualization","text":"<pre><code>import matplotlib.pyplot as plt\nimport numpy as np\n\ndef plot_comparison(report_spatial, report_sampling, model_list):\n    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n    pred_name = list(report_spatial.keys())[0]\n    spatial_scores = [report_spatial[pred_name].get(m, 0) for m in model_list]\n    sampling_scores = [report_sampling[pred_name].get(m, 0) for m in model_list]\n\n    x = np.arange(len(model_list))\n    colors = ['#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7', '#DDA0DD', '#FF6B6B']\n\n    axes[0].bar(x, spatial_scores, color=colors)\n    axes[0].set_title('Unseen Spatial')\n    axes[0].set_xticks(x)\n    axes[0].set_xticklabels(model_list, rotation=45)\n\n    axes[1].bar(x, sampling_scores, color=colors)\n    axes[1].set_title('Unseen Sampling')\n    axes[1].set_xticks(x)\n    axes[1].set_xticklabels(model_list, rotation=45)\n\n    plt.tight_layout()\n    plt.show()\n\nplot_comparison(report_spatial, report_sampling, model_list)\n</code></pre>"},{"location":"guide/comparison/#key-insights","title":"Key Insights","text":""},{"location":"guide/comparison/#unseen-spatial-new-locations","title":"Unseen Spatial (New Locations)","text":"<ul> <li>Interpolation methods (IDW) leverage spatial autocorrelation</li> <li>TwoStageModel captures both global density effect and local patterns</li> <li>Traditional ML often struggles without spatial structure</li> </ul>"},{"location":"guide/comparison/#unseen-sampling-new-density-levels","title":"Unseen Sampling (New Density Levels)","text":"<ul> <li>GAM excels at capturing density\u2192accuracy relationship</li> <li>TwoStageModel combines density modeling with spatial residuals</li> <li>Linear/SVM fail to generalize to unseen density ranges</li> </ul>"},{"location":"guide/comparison/#twostagemodel-advantage","title":"TwoStageModel Advantage","text":"<p>Decomposes the problem into interpretable components: - Stage 1: Global density effect (monotonic relationship) - Stage 2: Location-specific residuals (spatial patterns)</p> <p>This decomposition provides: 1. Better generalization to new conditions 2. Interpretable insights about accuracy drivers 3. Separate modeling of different sources of variation</p>"},{"location":"guide/splitting/","title":"Data Splitting","text":"<p>Proper data splitting is crucial for valid spatial model evaluation. GeoEquity provides several strategies to prevent data leakage.</p>"},{"location":"guide/splitting/#splitting-strategies","title":"Splitting Strategies","text":""},{"location":"guide/splitting/#site-wise-splitting","title":"Site-wise Splitting","text":"<p>Ensures all observations from the same monitoring site stay together:</p> <pre><code>from geoequity.data import split_test_train\n\ntrain_sites, test_sites, train_idx, test_idx, df = split_test_train(\n    df, split=0.2, flag='Site', seed=42\n)\n</code></pre> <p>Use when: You have repeated measurements at fixed locations.</p>"},{"location":"guide/splitting/#grid-wise-splitting","title":"Grid-wise Splitting","text":"<p>Divides the spatial domain into a grid and assigns entire grid cells to train/test:</p> <pre><code>_, _, train_idx, test_idx, df = split_test_train(\n    df, split=0.2, flag='Grid', grid_size=10, seed=42\n)\n</code></pre> <p>Use when: You want to test spatial extrapolation ability.</p>"},{"location":"guide/splitting/#spatiotemporal-splitting","title":"Spatiotemporal Splitting","text":"<p>Prevents leakage in both space and time:</p> <pre><code>_, _, train_idx, test_idx, df = split_test_train(\n    df, split=0.2, flag='Spatiotemporal', seed=42\n)\n</code></pre> <p>Use when: You have time series at multiple locations.</p>"},{"location":"guide/splitting/#cross-validation","title":"Cross-Validation","text":"<p>All strategies support k-fold cross-validation:</p> <pre><code>train_list, test_list, train_idx_list, test_idx_list, df = split_test_train(\n    df, cv=5, flag='Site', seed=42\n)\n\nfor fold, (train_idx, test_idx) in enumerate(zip(train_idx_list, test_idx_list)):\n    print(f\"Fold {fold}: {len(train_idx)} train, {len(test_idx)} test\")\n</code></pre>"},{"location":"guide/splitting/#why-spatial-splitting-matters","title":"Why Spatial Splitting Matters","text":"<p>Standard random splitting can cause:</p> <ul> <li>Spatial autocorrelation leakage: Nearby points in train/test sets</li> <li>Overly optimistic accuracy: Model appears better than it is</li> <li>Poor generalization: Fails on truly unseen regions</li> </ul> <p>Site-wise and grid-wise splitting prevent these issues by ensuring spatial separation between train and test sets.</p>"},{"location":"guide/theory/","title":"Theory","text":""},{"location":"guide/theory/#what-were-solving","title":"What We're Solving","text":"<p>You've trained an ML model and computed \\(R^2\\) at each validation station. Now you want to know:</p> <ol> <li>Why does accuracy vary across locations?</li> <li>What accuracy can I expect at a new location?</li> <li>How will accuracy change if I collect more data?</li> </ol> <p>The challenge: you only have \\(R^2\\) values at monitored stations\u2014how do you estimate accuracy everywhere else?</p>"},{"location":"guide/theory/#the-two-stage-approach","title":"The Two-Stage Approach","text":"<p>We model accuracy as depending on two factors:</p> Factor What it captures Data availability Density of stations, training sample size Location Geographic patterns not explained by data availability <pre><code># Input: raw test data\ndf_test: 100,000 rows \u2192 (lon, lat, density, sufficiency, observed, predicted)\n\n# Goal: predict R\u00b2 at any (lon, lat, density, sufficiency)\n</code></pre>"},{"location":"guide/theory/#stage-1-data-availability-accuracy","title":"Stage 1: Data Availability \u2192 Accuracy","text":"<p>We fit a GAM (Generalized Additive Model) to predict \\(R^2\\) from:</p> <ul> <li>Station density (\\(\\rho\\)): How many nearby stations exist</li> <li>Sample size (\\(n\\)): How much training data was used</li> </ul> <p>The model enforces a sensible constraint: more data = better accuracy. This is done through monotonic splines that can only increase with \\(\\rho\\) and \\(n\\).</p> <p>Output: A baseline accuracy estimate \\(\\hat{R}^2_{\\text{GAM}}\\) that captures global trends.</p> <pre><code>df_test \n  \u2192 Given: [sufficiency, lon, lat, observed, predicted, density]\n\n  # Remove local spatial noise first.\n  \u2192 Spatial Groupby (lon, lat)\n  \u2192 [sufficiency, lon_avg, lat_avg, density_avg, R\u00b2_compute]\n\n  # Extract sampling effect uniformly\n  \u2192 Sampling Groupby (sufficiency, density)\n  \u2192 [sufficiency, density_avg_bin, density_avg_avg, R\u00b2_avg] \n\n  # Then, GAM can learn the information of data availability in a uniform way\n\n  \u2192 Build GAM(density, sufficiency) \u2192 R\u00b2\n</code></pre>"},{"location":"guide/theory/#stage-2-location-specific-adjustments","title":"Stage 2: Location-Specific Adjustments","text":"<p>After Stage 1, some locations  still have unexplained accuracy differences. These residuals:</p> \\[r = R^2_{\\text{observed}} - \\hat{R}^2_{\\text{GAM}}\\] <p>are modeled using an SVM that learns spatial patterns\u2014capturing things like:</p> <ul> <li>Regional data quality differences</li> <li>Terrain complexity effects</li> <li>Local climate variability</li> </ul> <p>Output: A spatial correction \\(\\hat{r}_{\\text{SVM}}\\) for each location.</p> <p>\u2192 Group by [lon_bin, lat_bin, suff_bin]   \u2192 Get [(lon_bin, lat_bin), density, sufficiency, R\u00b2]   \u2192 baseline_r2 = GAM(density, sufficiency)   \u2192 residual = R\u00b2 - baseline_r2   \u2192 Build SVM(lon, lat) \u2192 residual</p> <pre><code>df_test \n  \u2192 Given: [sufficiency, lon, lat, observed, predicted, density]\n\n  # Remove local spatial noise.\n  \u2192 Spatial Groupby (lon, lat)\n  \u2192 [sufficiency, lon_avg, lat_avg, density_avg, R\u00b2_compute]\n\n  # Use model in stage 1\n  \u2192 residual_r2 = R\u00b2 - GAM(sufficiency, density)\n\n  # Then, SVM can learn the spatial pattern of residuals.\n  \u2192 Build SVM(lon_bin, lat_bin) \u2192 residual_r2\n</code></pre>"},{"location":"guide/theory/#final-prediction","title":"Final Prediction","text":"<p>Combine both stages:</p> \\[\\hat{R}^2 = \\hat{R}^2_{\\text{GAM}} + \\hat{r}_{\\text{SVM}}\\] Component Interprets Extrapolates to Stage 1 Density-accuracy relationship New data conditions Stage 2 Spatial residuals New locations Combined Full accuracy surface Anywhere"},{"location":"guide/theory/#why-this-works-better-than-interpolation","title":"Why This Works Better Than Interpolation","text":"<p>Traditional approach: interpolate \\(R^2\\) based on distance to nearby stations.</p> <p>Problem: This assumes accuracy varies smoothly with location, ignoring that sparse regions systematically underperform regardless of what's nearby.</p> <p>Our approach: Accuracy depends on data availability first, then location. A sparse region will have low predicted accuracy even if surrounded by high-accuracy stations.</p>"},{"location":"guide/theory/#citation","title":"Citation","text":"<pre><code>@article{liang2025geoequity,\n  title={Countering Local Overfitting for Equitable Spatiotemporal Modeling},\n  author={Liang, Zhehao and Castruccio, Stefano and Crippa, Paola},\n  journal={...},\n  year={2025}\n}\n</code></pre>"},{"location":"guide/two-stage/","title":"Two-Stage Model","text":"<p>The Two-Stage Model is GeoEquity's core method for predicting and understanding spatial variations in model accuracy.</p>"},{"location":"guide/two-stage/#how-it-works","title":"How It Works","text":""},{"location":"guide/two-stage/#stage-1-density-based-accuracy-gam","title":"Stage 1: Density-Based Accuracy (GAM)","text":"<p>The first stage uses a Monotonic Generalized Additive Model (GAM) to capture the relationship between data density and model accuracy.</p> <ul> <li>Input: Density features (sparsity, sufficiency)</li> <li>Model: Monotonic GAM with shape constraints</li> <li>Output: Baseline accuracy prediction</li> </ul>"},{"location":"guide/two-stage/#stage-2-spatial-residuals-svm","title":"Stage 2: Spatial Residuals (SVM)","text":"<p>The second stage uses a Support Vector Machine (SVM) to capture spatial patterns in the residuals from Stage 1.</p> <ul> <li>Input: Spatial coordinates (longitude, latitude)</li> <li>Model: SVM with RBF kernel</li> <li>Output: Spatial adjustment to accuracy</li> </ul>"},{"location":"guide/two-stage/#experiment-modes","title":"Experiment Modes","text":""},{"location":"guide/two-stage/#single-sufficiency-mode","title":"Single Sufficiency Mode","text":"<p>Use full dataset as one sample size level:</p> <pre><code>EXPERIMENT_MODE = 'single'\n\n# Sample to 100k if dataset is larger\nn_suff = min(100000, len(df))\ndf = df.sample(n=n_suff, random_state=42).copy()\ndf['sufficiency'] = 100000\n</code></pre> <p>Use when: Quick analysis, production models with fixed training size.</p>"},{"location":"guide/two-stage/#multi-sufficiency-mode","title":"Multi Sufficiency Mode","text":"<p>Study how accuracy changes with data volume:</p> <pre><code>EXPERIMENT_MODE = 'multi'\nSUFFICIENCY_LEVELS = [5000, 20000, 100000]\n\ndf_list = []\nfor suff in SUFFICIENCY_LEVELS:\n    df_sampled = df.sample(n=suff, random_state=42).copy()\n    df_sampled['sufficiency'] = suff\n    df_list.append(df_sampled)\ndf = pd.concat(df_list, ignore_index=True)\n</code></pre> <p>Use when: Research on data efficiency, understanding sampling effects.</p>"},{"location":"guide/two-stage/#usage","title":"Usage","text":"<pre><code>from geoequity import TwoStageModel\nfrom geoequity.two_stage.model import find_bins_intervals\n\n# Create density bins for aggregation\nbins_intervals = find_bins_intervals(df, density_bins=7)\n\n# Initialize model\nts_model = TwoStageModel(\n    spline=7,           # Number of spline bases for GAM\n    lam=0.5,            # GAM regularization\n    resolution=[30, 30] # Spatial aggregation grid\n)\n\n# Fit the model on test data\nts_model.fit(\n    df_train_raw=df.loc[test_idx],\n    model_name='linear',           # Matches 'predicted_linear' column\n    bins_intervals=bins_intervals,\n    split_by='grid'\n)\n\nprint(f\"Stage 1 R\u00b2: {ts_model.stage1_score:.4f}\")\nprint(f\"Stage 2 R\u00b2: {ts_model.stage2_score:.4f}\")\n</code></pre>"},{"location":"guide/two-stage/#prediction","title":"Prediction","text":"<pre><code>from geoequity.two_stage import predict_at_locations, plot_predicted_accuracy_map\n\n# Get station network info\nstations = df.drop_duplicates(subset=['longitude', 'latitude'])\nsufficiency = df['sufficiency'].iloc[0]\n\n# Predict at specific coordinates\nr2, density = predict_at_locations(\n    ts_model,\n    longitude=5.0,\n    latitude=50.0,\n    station_lons=stations['longitude'],\n    station_lats=stations['latitude'],\n    sufficiency=sufficiency\n)\n\n# Plot accuracy map\nfig, axes = plot_predicted_accuracy_map(\n    ts_model,\n    stations['longitude'].values,\n    stations['latitude'].values,\n    sufficiency,\n    lon_range=(-10, 35),\n    lat_range=(35, 70),\n    grid_size=30\n)\n</code></pre>"},{"location":"guide/two-stage/#diagnostic-outputs","title":"Diagnostic Outputs","text":"<pre><code>ts_model.diagnose(save_dir='diagnostics/', show=True)\n</code></pre> <p>Generates:</p> <ol> <li>stage1_gam.png: GAM partial dependence plots showing density\u2192accuracy relationship</li> <li>stage2_svm.png: Spatial map of residual patterns</li> <li>diagnosis.txt: Summary statistics</li> </ol> <p>Example output: <pre><code>Stage 1 (Monotonic GAM) - Density/Sampling Effect:\n  - r = 0.9872, MAE = 0.0214\n\nStage 2 (SVM) - Spatial Residual:\n  - r = 0.4986, MAE = 0.1830\n\nKey Insights:\n  1. Density effect captured in Stage 1 (r=0.987)\n  2. Spatial pattern captured in Stage 2 (r=0.499)\n  3. Prediction at unseen locations: r=0.643 (Total)\n</code></pre></p>"},{"location":"guide/two-stage/#interpretation","title":"Interpretation","text":"Pattern Meaning High Stage 1 R\u00b2 Density strongly predicts accuracy High Stage 2 residuals Location-specific factors beyond density Clustered Stage 2 patterns Geographic regions with systematic bias Uniform Stage 2 Density alone explains accuracy variations"},{"location":"guide/two-stage/#key-insight","title":"Key Insight","text":"<p>Machine learning models trained on geospatial data exhibit spatial inequity: prediction accuracy varies systematically across space due to:</p> <ol> <li>Sampling Density Effect (Stage 1): Denser observation networks \u2192 more training data \u2192 higher accuracy</li> <li>Spatial Residual Effect (Stage 2): Location-specific data quality, terrain complexity, etc.</li> </ol> <p>TwoStageModel quantifies this inequity, enabling fair assessment and identification of under-served regions.</p>"},{"location":"guide/visualization/","title":"Visualization","text":"<p>GeoEquity provides tools for creating publication-ready spatial accuracy visualizations.</p>"},{"location":"guide/visualization/#visualization-modes","title":"Visualization Modes","text":"Mode Description Use Case <code>observation</code> Per-station R\u00b2 scatter Show raw accuracy at observation locations <code>interpolation</code> IDW interpolation Smooth accuracy surface <code>average_NxM</code> Grid-averaged R\u00b2 Aggregate accuracy in spatial bins <code>spatial_model</code> SVM regression ML-based spatial pattern <code>two_stage</code> TwoStageModel prediction Density-aware accuracy prediction"},{"location":"guide/visualization/#twostagemodel-prediction-map","title":"TwoStageModel Prediction Map","text":"<pre><code>from geoequity.two_stage.visualization import plot_predicted_accuracy_map\n\n# Get station network\nstations = df.drop_duplicates(subset=['longitude', 'latitude'])\nsufficiency = df['sufficiency'].iloc[0]\n\n# Plot density + predicted accuracy\nfig, axes = plot_predicted_accuracy_map(\n    ts_model,\n    stations['longitude'].values,\n    stations['latitude'].values,\n    sufficiency,\n    lon_range=(-10, 35),\n    lat_range=(35, 70),\n    grid_size=30\n)\nplt.suptitle('TwoStageModel: Density \u2192 Accuracy', y=1.02)\nplt.show()\n</code></pre>"},{"location":"guide/visualization/#compare-multiple-modes","title":"Compare Multiple Modes","text":"<pre><code>from geoequity.visualization import plot_accuracy_comparison\n\nfig, axes = plot_accuracy_comparison(\n    df.loc[test_idx],\n    model_name='linear',\n    modes=['observation', 'interpolation', 'average_15x20', 'spatial_model', 'two_stage'],\n    accuracy_range=(0, 1),\n    lon_range=(-10, 35),\n    lat_range=(35, 70),\n    # Required for 'two_stage' mode:\n    ts_model=ts_model,\n    station_lons=stations['longitude'].values,\n    station_lats=stations['latitude'].values\n)\nplt.suptitle('Model Accuracy - Visualization Modes', y=1.02)\nplt.show()\n</code></pre>"},{"location":"guide/visualization/#single-mode-plot","title":"Single Mode Plot","text":"<pre><code>from geoequity.visualization import plot_accuracy_map\n\nfig, ax = plot_accuracy_map(\n    df.loc[test_idx],\n    model_name='linear',\n    mode='interpolation',\n    accuracy_range=(0, 1),\n    lon_range=(-10, 35),\n    lat_range=(35, 70)\n)\nplt.show()\n</code></pre>"},{"location":"guide/visualization/#diagnostic-plots","title":"Diagnostic Plots","text":"<pre><code># Generate TwoStageModel diagnostics\nts_model.diagnose(save_dir='diagnostics/', show=True)\n</code></pre> <p>Outputs: - stage1_gam.png: GAM partial dependence plots - stage2_svm.png: Spatial residual map - diagnosis.txt: Summary statistics</p>"},{"location":"guide/visualization/#note-on-negative-r2-values","title":"Note on Negative R\u00b2 Values","text":"<p>R\u00b2 can be negative when the model performs worse than predicting the mean. This happens when: - Sparse regions have insufficient data - Model has poor generalization in certain areas</p> <p>If negative R\u00b2 values appear frequently, consider:</p> Alternative Metric Description MSE/RMSE Always positive, easier to interpret MAE More robust to outliers Correlation (r) Ranges -1 to 1, captures linear relationship"},{"location":"guide/visualization/#styling-tips","title":"Styling Tips","text":"<ul> <li>Use <code>Spectral_r</code> or <code>RdYlGn</code> for accuracy (red=low, green=high)</li> <li>Set consistent <code>accuracy_range=(0, 1)</code> for R\u00b2 values</li> <li>Add land boundaries with geopandas for context</li> <li>Use <code>grid_size=30</code> for smooth maps, lower for faster rendering</li> </ul>"}]}