{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"GeoEquity","text":"<p>Spatial Equity Assessment for Machine Learning Models</p> <p>GeoEquity provides tools to diagnose and visualize spatial performance disparities in geospatial machine learning models. It helps identify where models underperform due to data sparsity and provides methods to predict and visualize accuracy across space.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>\ud83c\udfaf Two-Stage Accuracy Prediction: Predict model accuracy using density features (Stage 1) and spatial patterns (Stage 2)</li> <li>\ud83d\udcca Spatial Equity Diagnostics: Identify regions where models underperform</li> <li>\ud83d\uddfa\ufe0f Visualization Tools: Create publication-ready accuracy maps</li> <li>\u2702\ufe0f Smart Data Splitting: Site-wise, grid-wise, and spatiotemporal splitting strategies</li> </ul>"},{"location":"#quick-example","title":"Quick Example","text":"<pre><code>from geoequity import TwoStageModel\nfrom geoequity.data import split_test_train, calculate_density\n\n# Load and prepare data\ndf = calculate_density(df, radius=500)\ntrain_sites, test_sites, train_idx, test_idx, df = split_test_train(\n    df, split=0.2, flag='Site', seed=42\n)\n\n# Fit two-stage model to analyze spatial accuracy\nts = TwoStageModel(resolution=[30, 30])\nts.fit(df.loc[test_idx], model_col='predicted_mymodel')\n\n# Predict accuracy for any location\naccuracy = ts.predict(longitude=5.0, latitude=50.0, density=0.001)\n\n# Generate diagnostic report\nts.diagnose(save_dir='diagnostics/')\n</code></pre>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install geoequity\n</code></pre>"},{"location":"#documentation","title":"Documentation","text":"<ul> <li>Getting Started - Installation and first steps</li> <li>User Guide - Detailed usage instructions</li> <li>API Reference - Complete API documentation</li> <li>Examples - Jupyter notebook tutorials</li> </ul>"},{"location":"#citation","title":"Citation","text":"<p>If you use GeoEquity in your research, please cite:</p> <pre><code>@article{liang2025geoequity,\n  title={Countering Local Overfitting for Equitable Spatiotemporal Modeling},\n  author={Liang, Zhehao and Castruccio, Stefano and Crippa, Paola},\n  journal={...},\n  year={2025}\n}\n</code></pre>"},{"location":"#license","title":"License","text":"<p>MIT License</p>"},{"location":"api/data/","title":"Data Utilities","text":""},{"location":"api/data/#split_test_train","title":"split_test_train","text":"<p>Split data into train/test sets with spatial awareness.</p>"},{"location":"api/data/#usage","title":"Usage","text":"<pre><code>from geoequity.data import split_test_train\n\n# Single split\ntrain_sites, test_sites, train_idx, test_idx, df = split_test_train(\n    df, split=0.2, flag='Site', seed=42\n)\n\n# Cross-validation\ntrain_list, test_list, train_idx_list, test_idx_list, df = split_test_train(\n    df, cv=5, flag='Site', seed=42\n)\n</code></pre>"},{"location":"api/data/#parameters","title":"Parameters","text":"Name Type Default Description df DataFrame - Input data split float 0.2 Test set fraction flag str 'Site' Splitting strategy: 'Site', 'Grid', 'Spatiotemporal' cv int None Number of CV folds (overrides split) seed int 42 Random seed grid_size int 10 Grid cells per dimension (for Grid flag)"},{"location":"api/data/#calculate_density","title":"calculate_density","text":"<p>Calculate data density for each observation based on nearby points.</p>"},{"location":"api/data/#usage_1","title":"Usage","text":"<pre><code>from geoequity.data import calculate_density\n\ndf = calculate_density(df, radius=500)\n</code></pre>"},{"location":"api/data/#parameters_1","title":"Parameters","text":"Name Type Default Description df DataFrame - Input data with lon/lat columns radius float 500 Search radius in kilometers lon_col str 'longitude' Longitude column name lat_col str 'latitude' Latitude column name"},{"location":"api/data/#returns","title":"Returns","text":"<p>DataFrame with added <code>density</code> column.</p>"},{"location":"api/two-stage/","title":"TwoStageModel","text":"<p>The core model for predicting spatial variations in ML model accuracy.</p> <pre><code>from geoequity import TwoStageModel\n\nmodel = TwoStageModel(spline=7, lam=0.5, resolution=[30, 30])\n</code></pre>"},{"location":"api/two-stage/#constructor","title":"Constructor","text":"Parameter Type Default Description spline int 7 Number of spline bases for GAM lam float 0.5 GAM regularization parameter resolution list [30, 30] Grid resolution for SVM"},{"location":"api/two-stage/#methods","title":"Methods","text":""},{"location":"api/two-stage/#fit","title":"fit","text":"<pre><code>def fit(self, df, model_col, target_col='observed', \n        density_col='density', lon_col='longitude', lat_col='latitude'):\n</code></pre> <p>Train the two-stage model on test data.</p> <p>Parameters:</p> Name Type Description df DataFrame Test data with predictions and ground truth model_col str Column name containing model predictions target_col str Column name containing ground truth values density_col str Column name containing density values lon_col str Column name for longitude lat_col str Column name for latitude"},{"location":"api/two-stage/#predict","title":"predict","text":"<pre><code>def predict(self, longitude, latitude, density):\n</code></pre> <p>Predict accuracy for given coordinates and density.</p> <p>Parameters:</p> Name Type Description longitude float or array Longitude(s) to predict latitude float or array Latitude(s) to predict density float or array Density value(s) <p>Returns: Predicted R\u00b2 score(s)</p>"},{"location":"api/two-stage/#diagnose","title":"diagnose","text":"<pre><code>def diagnose(self, save_dir='diagnostics/', show=True):\n</code></pre> <p>Generate diagnostic plots and summary report.</p> <p>Parameters:</p> Name Type Description save_dir str Directory to save outputs show bool Whether to display plots <p>Outputs: - <code>stage1_gam.png</code>: GAM partial dependence plots - <code>stage2_svm.png</code>: Spatial residual map - <code>diagnosis.txt</code>: Summary statistics</p>"},{"location":"api/visualization/","title":"Visualization","text":""},{"location":"api/visualization/#plot_accuracy_map","title":"plot_accuracy_map","text":"<p>Plot predicted accuracy as a spatial map.</p>"},{"location":"api/visualization/#usage","title":"Usage","text":"<pre><code>from geoequity.visualization import plot_accuracy_map\n\nfig, ax = plot_accuracy_map(\n    model,\n    ds,\n    mask=land_mask,\n    cmap='Spectral_r',\n    vmin=0, vmax=1\n)\n</code></pre>"},{"location":"api/visualization/#parameters","title":"Parameters","text":"Name Type Default Description model TwoStageModel - Fitted TwoStageModel ds xarray.Dataset - Dataset with lon/lat coordinates mask array None Boolean mask for valid regions cmap str 'Spectral_r' Matplotlib colormap vmin float 0 Colorbar minimum vmax float 1 Colorbar maximum"},{"location":"api/visualization/#plot_accuracy_comparison","title":"plot_accuracy_comparison","text":"<p>Compare accuracy across multiple visualization modes.</p>"},{"location":"api/visualization/#usage_1","title":"Usage","text":"<pre><code>from geoequity.visualization import plot_accuracy_comparison\n\nfig, axes = plot_accuracy_comparison(\n    df,\n    model_name='MyModel',\n    modes=['observation', 'interpolation', 'spatial_model'],\n    accuracy_range=(0, 1),\n    lon_range=(-10, 35),\n    lat_range=(35, 70)\n)\n</code></pre>"},{"location":"api/visualization/#parameters_1","title":"Parameters","text":"Name Type Default Description df DataFrame - Data with model predictions model_name str - Model column prefix modes list - Visualization modes to compare accuracy_range tuple (0, 1) Colorbar range lon_range tuple None Longitude bounds lat_range tuple None Latitude bounds"},{"location":"examples/Standard_workflow_geoequity/","title":"TwoStageModel: Spatial Equity Diagnostic","text":"In\u00a0[89]: Copied! <pre>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\nimport sys\nfrom pathlib import Path\n%load_ext autoreload\n%autoreload 2\n\n# Add geoequity to path (for running without pip install)\n# This allows running directly from source code\nGEOEQUITY_ROOT = Path(\"..\").resolve()\nif str(GEOEQUITY_ROOT) not in sys.path:\n    sys.path.insert(0, str(GEOEQUITY_ROOT))\n\n# Import geoequity modules\nfrom two_stage import TwoStageModel\nfrom data import split_test_train, calculate_density, simple_feature_engineering\n\nprint(\"geoequity imported successfully!\")\nprint(f\"Source path: {GEOEQUITY_ROOT}\")\n</pre> import numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn.linear_model import LinearRegression from sklearn.metrics import r2_score import sys from pathlib import Path %load_ext autoreload %autoreload 2  # Add geoequity to path (for running without pip install) # This allows running directly from source code GEOEQUITY_ROOT = Path(\"..\").resolve() if str(GEOEQUITY_ROOT) not in sys.path:     sys.path.insert(0, str(GEOEQUITY_ROOT))  # Import geoequity modules from two_stage import TwoStageModel from data import split_test_train, calculate_density, simple_feature_engineering  print(\"geoequity imported successfully!\") print(f\"Source path: {GEOEQUITY_ROOT}\")  <pre>The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\ngeoequity imported successfully!\nSource path: D:\\OneDrive\\Code\\Ozone_Reconstruction\\Submission_Code\\geoequity\n</pre> In\u00a0[90]: Copied! <pre># ============================================================\n# \ud83c\udfaf CONFIGURATION - Modify these settings for your dataset\n# ============================================================\n\n# Experiment mode\nEXPERIMENT_MODE = 'single'  # Options: 'single' or 'multi'\n# - 'single': Use full dataset (one sufficiency level)\n# - 'multi':  Create 3 sufficiency levels: 5000, 20000, 100000\n\n# Data path\nDATA_PATH = 'data/df_example.pkl'  # Replace with your data file\n\n# Target column (what you want to predict)\nTARGET_COL = 'Ozone'\n\n# Feature columns (predictors for baseline model)\nFEATURE_COLS = ['time','TROPOMI_ozone', 'OMI_ozone', 'tco3', 'blh', 't2m', 'sp', 'WS10', \n 'population', 'no2', 'DSR',\"strd\", \"r_1000\", \"lai_hv\", \"pev\", \"ssro\",\"t_975\",\"t_925\",\"tp\",\"tsn\",\"stl1\",'latitude','longitude']\n\n# Prediction column name (will be created by baseline model)\nPRED_COL = 'predicted_linear'\n</pre>   # ============================================================ # \ud83c\udfaf CONFIGURATION - Modify these settings for your dataset # ============================================================  # Experiment mode EXPERIMENT_MODE = 'single'  # Options: 'single' or 'multi' # - 'single': Use full dataset (one sufficiency level) # - 'multi':  Create 3 sufficiency levels: 5000, 20000, 100000  # Data path DATA_PATH = 'data/df_example.pkl'  # Replace with your data file  # Target column (what you want to predict) TARGET_COL = 'Ozone'  # Feature columns (predictors for baseline model) FEATURE_COLS = ['time','TROPOMI_ozone', 'OMI_ozone', 'tco3', 'blh', 't2m', 'sp', 'WS10',   'population', 'no2', 'DSR',\"strd\", \"r_1000\", \"lai_hv\", \"pev\", \"ssro\",\"t_975\",\"t_925\",\"tp\",\"tsn\",\"stl1\",'latitude','longitude']  # Prediction column name (will be created by baseline model) PRED_COL = 'predicted_linear'  In\u00a0[91]: Copied! <pre># ============================================================\n# Load data\n# ============================================================\ndf = pd.read_pickle(DATA_PATH)\ndf[\"time\"]=pd.to_datetime(df[\"time\"])\nfor col in df.select_dtypes(include=[\"float64\", \"int64\"]).columns:\n    df[col] = df[col].astype(\"float32\")\n    \n# Convert to float32 for memory efficiency\nfor col in df.select_dtypes(include=['float64']).columns:\n    df[col] = df[col].astype('float32')\n\n# Validate required columns\nrequired_cols = ['longitude', 'latitude', 'time', TARGET_COL]\nmissing = [c for c in required_cols if c not in df.columns]\nif missing:\n    raise ValueError(f\"Missing required columns: {missing}\")\n\n# Validate feature columns\navailable_features = [c for c in FEATURE_COLS if c in df.columns]\nif len(available_features) &lt; len(FEATURE_COLS):\n    print(f\"\u26a0\ufe0f Some features not found, using: {available_features}\")\n    FEATURE_COLS = available_features\n\nn_locations = df.groupby(['longitude', 'latitude']).ngroup().nunique()\nprint(f\"Loaded {len(df):,} samples from {n_locations:,} unique locations\")\nprint(f\"Time range: {df['time'].min()} to {df['time'].max()}\")\nprint(f\"\\nTarget: {TARGET_COL}\")\nprint(f\"Features: {FEATURE_COLS}\")\ndf.head()\n</pre>  # ============================================================ # Load data # ============================================================ df = pd.read_pickle(DATA_PATH) df[\"time\"]=pd.to_datetime(df[\"time\"]) for col in df.select_dtypes(include=[\"float64\", \"int64\"]).columns:     df[col] = df[col].astype(\"float32\")      # Convert to float32 for memory efficiency for col in df.select_dtypes(include=['float64']).columns:     df[col] = df[col].astype('float32')  # Validate required columns required_cols = ['longitude', 'latitude', 'time', TARGET_COL] missing = [c for c in required_cols if c not in df.columns] if missing:     raise ValueError(f\"Missing required columns: {missing}\")  # Validate feature columns available_features = [c for c in FEATURE_COLS if c in df.columns] if len(available_features) &lt; len(FEATURE_COLS):     print(f\"\u26a0\ufe0f Some features not found, using: {available_features}\")     FEATURE_COLS = available_features  n_locations = df.groupby(['longitude', 'latitude']).ngroup().nunique() print(f\"Loaded {len(df):,} samples from {n_locations:,} unique locations\") print(f\"Time range: {df['time'].min()} to {df['time'].max()}\") print(f\"\\nTarget: {TARGET_COL}\") print(f\"Features: {FEATURE_COLS}\") df.head() <pre>Loaded 3,520,452 samples from 1,302 unique locations\nTime range: 2019-06-01 00:00:00 to 2019-09-29 23:00:00\n\nTarget: Ozone\nFeatures: ['time', 'TROPOMI_ozone', 'OMI_ozone', 'tco3', 'blh', 't2m', 'sp', 'WS10', 'population', 'no2', 'DSR', 'strd', 'r_1000', 'lai_hv', 'pev', 'ssro', 't_975', 't_925', 'tp', 'tsn', 'stl1', 'latitude', 'longitude']\n</pre> Out[91]: time r_1000 sp stl1 blh t_975 population tsn t2m DSR ... tp no2 strd OMI_ozone Unnamed: 0 site_number sampling_height Ozone latitude longitude 4235938 2019-06-01 00:00:00 40.118874 92562.484375 294.035126 12.997529 301.286194 0.0 273.159912 294.071289 323.755768 ... 0.000003 0.000012 30846750.0 0.000007 3521.0 1.0 2.75 56.968533 35.0 33.0 4235939 2019-06-01 01:00:00 40.820999 92550.921875 293.471710 11.205620 301.448181 0.0 273.159668 293.956604 323.060455 ... 0.000000 0.000013 1177312.0 0.000007 3522.0 1.0 2.75 60.438416 35.0 33.0 4235940 2019-06-01 02:00:00 39.497925 92548.421875 292.972778 13.652981 301.544983 0.0 273.159668 293.848602 322.365143 ... 0.000000 0.000013 2347693.5 0.000007 3523.0 1.0 2.75 59.958427 35.0 33.0 4235941 2019-06-01 03:00:00 37.452812 92548.109375 292.592804 19.237371 301.245117 0.0 273.161133 294.628082 321.669861 ... 0.000000 0.000013 3514501.0 0.000006 3524.0 1.0 2.75 56.148556 35.0 33.0 4235942 2019-06-01 04:00:00 36.479145 92570.617188 293.640198 35.947742 300.535797 0.0 273.159668 295.702423 320.974548 ... 0.000000 0.000014 4685279.5 0.000006 3525.0 1.0 2.75 56.758514 35.0 33.0 <p>5 rows \u00d7 29 columns</p> In\u00a0[92]: Copied! <pre># ============================================================\n# Create sufficiency levels based on experiment mode\n# ============================================================\nif EXPERIMENT_MODE == 'multi':\n    # Multi-sufficiency: sample at 3 different sizes\n    SUFFICIENCY_LEVELS = [5000, 20000, 100000]\n    \n    np.random.seed(42)\n    df_list = []\n    for suff in SUFFICIENCY_LEVELS:\n        if len(df) &gt;= suff:\n            df_sampled = df.sample(n=suff, random_state=42).copy()\n        else:\n            df_sampled = df.copy()\n        df_sampled['sufficiency'] = suff\n        df_list.append(df_sampled)\n    \n    df = pd.concat(df_list, ignore_index=True)\n    print(f\"Multi-sufficiency mode: {SUFFICIENCY_LEVELS}\")\n    print(f\"Total samples: {len(df):,}\")\n    print(f\"\\nSamples per sufficiency level:\")\n    print(df['sufficiency'].value_counts().sort_index())\n    \nelse:\n    # Single sufficiency: use 100k (or as many samples as available if less)\n    n_suff = min(100000, len(df))\n    if len(df) &gt; n_suff:\n        df = df.sample(n=n_suff, random_state=42).copy()\n    df['sufficiency'] = 100000\n    print(f\"Single sufficiency mode: using {len(df):,} samples (target 100,000)\")\n</pre> # ============================================================ # Create sufficiency levels based on experiment mode # ============================================================ if EXPERIMENT_MODE == 'multi':     # Multi-sufficiency: sample at 3 different sizes     SUFFICIENCY_LEVELS = [5000, 20000, 100000]          np.random.seed(42)     df_list = []     for suff in SUFFICIENCY_LEVELS:         if len(df) &gt;= suff:             df_sampled = df.sample(n=suff, random_state=42).copy()         else:             df_sampled = df.copy()         df_sampled['sufficiency'] = suff         df_list.append(df_sampled)          df = pd.concat(df_list, ignore_index=True)     print(f\"Multi-sufficiency mode: {SUFFICIENCY_LEVELS}\")     print(f\"Total samples: {len(df):,}\")     print(f\"\\nSamples per sufficiency level:\")     print(df['sufficiency'].value_counts().sort_index())      else:     # Single sufficiency: use 100k (or as many samples as available if less)     n_suff = min(100000, len(df))     if len(df) &gt; n_suff:         df = df.sample(n=n_suff, random_state=42).copy()     df['sufficiency'] = 100000     print(f\"Single sufficiency mode: using {len(df):,} samples (target 100,000)\")  <pre>Single sufficiency mode: using 100,000 samples (target 100,000)\n</pre> In\u00a0[93]: Copied! <pre># Calculate density with 500km radius\n# In real case, you should calculate the density based on training stations\ndf = calculate_density(df, radius=500)\n\nprint(f\"Density range: {df['density'].min():.2e} - {df['density'].max():.2e}\")\n\n# Visualize density distribution\nfig, ax = plt.subplots(figsize=(5, 3))\nscatter = ax.scatter(df['longitude'], df['latitude'], c=df['density'], \n                     cmap='viridis', s=3, alpha=0.5)\nplt.colorbar(scatter, label='Data Density (stations/km\u00b2)')\nax.set_xlabel('Longitude')\nax.set_ylabel('Latitude')\nax.set_title('Observation Station Density Distribution')\nplt.tight_layout()\nplt.show()\n</pre> # Calculate density with 500km radius # In real case, you should calculate the density based on training stations df = calculate_density(df, radius=500)  print(f\"Density range: {df['density'].min():.2e} - {df['density'].max():.2e}\")  # Visualize density distribution fig, ax = plt.subplots(figsize=(5, 3)) scatter = ax.scatter(df['longitude'], df['latitude'], c=df['density'],                       cmap='viridis', s=3, alpha=0.5) plt.colorbar(scatter, label='Data Density (stations/km\u00b2)') ax.set_xlabel('Longitude') ax.set_ylabel('Latitude') ax.set_title('Observation Station Density Distribution') plt.tight_layout() plt.show()  <pre>Calculating density (r=500km):   0%|          | 0/1302 [00:00&lt;?, ?it/s]</pre> <pre>Density range: 1.81e-06 - 2.11e-04\n</pre> In\u00a0[94]: Copied! <pre># Site-wise split using split_test_train\n# flag='Site' ensures same station's observations stay in same set\n\nif EXPERIMENT_MODE == 'multi':\n    # Multi: split each sufficiency level separately\n    split_results = {}  # {sufficiency: (train_idx, test_idx)}\n    for suff in df['sufficiency'].unique():\n        df_suff = df[df['sufficiency'] == suff].copy()\n        _, _, train_idx_s, test_idx_s, _ = split_test_train(\n            df_suff, split=0.2, flag='Site', seed=42, verbose=0\n        )\n        split_results[suff] = (train_idx_s, test_idx_s)\n        print(f\"Sufficiency {suff:,}: Train {len(train_idx_s)}, Test {len(test_idx_s)}\")\nelse:\n    # Single: original logic\n    train_sites, test_sites, train_idx, test_idx, df = split_test_train(\n        df, split=0.2, flag='Site', seed=42, verbose=1\n    )\n    print(f\"\\nTrain: {len(train_idx)} samples, Test: {len(test_idx)} samples\")\n</pre> # Site-wise split using split_test_train # flag='Site' ensures same station's observations stay in same set  if EXPERIMENT_MODE == 'multi':     # Multi: split each sufficiency level separately     split_results = {}  # {sufficiency: (train_idx, test_idx)}     for suff in df['sufficiency'].unique():         df_suff = df[df['sufficiency'] == suff].copy()         _, _, train_idx_s, test_idx_s, _ = split_test_train(             df_suff, split=0.2, flag='Site', seed=42, verbose=0         )         split_results[suff] = (train_idx_s, test_idx_s)         print(f\"Sufficiency {suff:,}: Train {len(train_idx_s)}, Test {len(test_idx_s)}\") else:     # Single: original logic     train_sites, test_sites, train_idx, test_idx, df = split_test_train(         df, split=0.2, flag='Site', seed=42, verbose=1     )     print(f\"\\nTrain: {len(train_idx)} samples, Test: {len(test_idx)} samples\")  <pre>Using flag: Site\nSelected Site Count: 260, (19.97%)\nSelected DataRow Count: 19765, (19.77%)\nTraining Site Count: 1042, (80.03%)\nTraining DataRow Count: 80235, (80.23%)\n\nTrain: 80235 samples, Test: 19765 samples\n</pre> In\u00a0[95]: Copied! <pre># ============================================================\n# Feature Engineering &amp; Train - per sufficiency level if multi\n# ============================================================\ndf[PRED_COL] = np.nan  # Initialize prediction column\n\nif EXPERIMENT_MODE == 'multi':\n    # Multi: train each sufficiency level separately\n    for suff in df['sufficiency'].unique():\n        print(f\"\\n{'='*50}\")\n        print(f\"Training for Sufficiency = {suff:,}\")\n        print(f\"{'='*50}\")\n        \n        # Get data for this sufficiency\n        df_suff = df[df['sufficiency'] == suff]\n        train_idx_s, test_idx_s = split_results[suff]\n        \n        # Feature engineering\n        X_suff, y_suff, _ = simple_feature_engineering(\n            df_suff, FEATURE_COLS, TARGET_COL,\n            add_spatial_harmonics=True, add_temporal_harmonics=True,\n            standardize=True, verbose=False\n        )\n        \n        X_train = X_suff.loc[train_idx_s]\n        y_train = y_suff.loc[train_idx_s]\n        X_test = X_suff.loc[test_idx_s]\n        y_test = y_suff.loc[test_idx_s]\n        \n        # Train model\n        model = LinearRegression()\n        model.fit(X_train, y_train)\n        \n        # Add predictions back to df\n        df.loc[train_idx_s, PRED_COL] = model.predict(X_train)\n        df.loc[test_idx_s, PRED_COL] = model.predict(X_test)\n        \n        # Evaluate\n        test_r2 = r2_score(y_test, df.loc[test_idx_s, PRED_COL])\n        print(f\"Test R\u00b2: {test_r2:.4f}\")\n    \n    # Store all test indices for TwoStageModel\n    all_test_idx = []\n    for suff in df['sufficiency'].unique():\n        all_test_idx.extend(split_results[suff][1])\n    test_idx = all_test_idx\n\nelse:\n    # Single: original logic\n    X, y, FINAL_FEATURES = simple_feature_engineering(\n        df, FEATURE_COLS, TARGET_COL,\n        add_spatial_harmonics=True, add_temporal_harmonics=True, standardize=True\n    )\n    \n    X_train = X.loc[train_idx]\n    y_train = y.loc[train_idx]\n    X_test = X.loc[test_idx]\n    y_test = y.loc[test_idx]\n    \n    print(f\"\\nTrain shape: {X_train.shape}, Test shape: {X_test.shape}\")\n    \n    model = LinearRegression()\n    model.fit(X_train, y_train)\n    \n    df.loc[train_idx, PRED_COL] = model.predict(X_train)\n    df.loc[test_idx, PRED_COL] = model.predict(X_test)\n    \n    train_r2 = r2_score(y_train, df.loc[train_idx, PRED_COL])\n    test_r2 = r2_score(y_test, df.loc[test_idx, PRED_COL])\n    \n    print(f\"\\n{'='*50}\")\n    print(f\"Model Evaluation\")\n    print(f\"{'='*50}\")\n    print(f\"Train R\u00b2: {train_r2:.4f}\")\n    print(f\"Test R\u00b2:  {test_r2:.4f}\")\n</pre> # ============================================================ # Feature Engineering &amp; Train - per sufficiency level if multi # ============================================================ df[PRED_COL] = np.nan  # Initialize prediction column  if EXPERIMENT_MODE == 'multi':     # Multi: train each sufficiency level separately     for suff in df['sufficiency'].unique():         print(f\"\\n{'='*50}\")         print(f\"Training for Sufficiency = {suff:,}\")         print(f\"{'='*50}\")                  # Get data for this sufficiency         df_suff = df[df['sufficiency'] == suff]         train_idx_s, test_idx_s = split_results[suff]                  # Feature engineering         X_suff, y_suff, _ = simple_feature_engineering(             df_suff, FEATURE_COLS, TARGET_COL,             add_spatial_harmonics=True, add_temporal_harmonics=True,             standardize=True, verbose=False         )                  X_train = X_suff.loc[train_idx_s]         y_train = y_suff.loc[train_idx_s]         X_test = X_suff.loc[test_idx_s]         y_test = y_suff.loc[test_idx_s]                  # Train model         model = LinearRegression()         model.fit(X_train, y_train)                  # Add predictions back to df         df.loc[train_idx_s, PRED_COL] = model.predict(X_train)         df.loc[test_idx_s, PRED_COL] = model.predict(X_test)                  # Evaluate         test_r2 = r2_score(y_test, df.loc[test_idx_s, PRED_COL])         print(f\"Test R\u00b2: {test_r2:.4f}\")          # Store all test indices for TwoStageModel     all_test_idx = []     for suff in df['sufficiency'].unique():         all_test_idx.extend(split_results[suff][1])     test_idx = all_test_idx  else:     # Single: original logic     X, y, FINAL_FEATURES = simple_feature_engineering(         df, FEATURE_COLS, TARGET_COL,         add_spatial_harmonics=True, add_temporal_harmonics=True, standardize=True     )          X_train = X.loc[train_idx]     y_train = y.loc[train_idx]     X_test = X.loc[test_idx]     y_test = y.loc[test_idx]          print(f\"\\nTrain shape: {X_train.shape}, Test shape: {X_test.shape}\")          model = LinearRegression()     model.fit(X_train, y_train)          df.loc[train_idx, PRED_COL] = model.predict(X_train)     df.loc[test_idx, PRED_COL] = model.predict(X_test)          train_r2 = r2_score(y_train, df.loc[train_idx, PRED_COL])     test_r2 = r2_score(y_test, df.loc[test_idx, PRED_COL])          print(f\"\\n{'='*50}\")     print(f\"Model Evaluation\")     print(f\"{'='*50}\")     print(f\"Train R\u00b2: {train_r2:.4f}\")     print(f\"Test R\u00b2:  {test_r2:.4f}\")  <pre>Feature Engineering Pipeline: fill NA \u2192 temporal harmonics \u2192 spatial harmonics \u2192 standardize\n  \u2713 Added temporal harmonics: T1, T2, T3, month, hour\n  \u2713 Added spatial harmonics: S1, S2, S3 (replaced lon/lat)\n  \u2713 Standardized all features\n\nFinal features (28): ['TROPOMI_ozone', 'OMI_ozone', 'tco3', 'blh', 't2m']...\n\nTrain shape: (80235, 28), Test shape: (19765, 28)\n\n==================================================\nModel Evaluation\n==================================================\nTrain R\u00b2: 0.4953\nTest R\u00b2:  0.4897\n</pre> In\u00a0[96]: Copied! <pre># ============================================================\n# Prepare data for TwoStageModel\n# ============================================================\nfrom two_stage.model import find_bins_intervals\n\n# Add 'observed' column (required by TwoStageModel)\ndf['observed'] = df[TARGET_COL]\n\n# Extract model name from prediction column (e.g., 'predicted_linear' -&gt; 'linear')\nMODEL_NAME = PRED_COL.replace('predicted_', '')\n\n# Create bins intervals for density and sufficiency\nbins_intervals = find_bins_intervals(df, density_bins=7)\nprint(f\"Created bins for {len(bins_intervals[1])} sufficiency levels\")\n\n# ============================================================\n# Initialize and fit TwoStageModel\n# ============================================================\nts_model = TwoStageModel(\n    spline=7,           # GAM spline knots\n    lam=0.5,            # GAM regularization\n    resolution=[30, 30] # Spatial aggregation grid\n)\n\n# Fit model using test data to analyze accuracy patterns\n# This learns how accuracy varies with density and location\n# In real case, you should use the density based on training stations\nts_model.fit(\n    df_train_raw=df.loc[test_idx],\n    model_name=MODEL_NAME,\n    bins_intervals=bins_intervals,\n    split_by='grid'\n)\n\nprint(f\"\\n{'='*50}\")\nprint(f\"TwoStageModel Training Complete!\")\nprint(f\"{'='*50}\")\nprint(f\"Model: {PRED_COL} \u2192 {TARGET_COL}\")\nprint(f\"Stage 1 R\u00b2 (GAM on density): {ts_model.stage1_score:.4f}\")\nprint(f\"Stage 2 R\u00b2 (GAM+SVM spatial): {ts_model.stage2_score:.4f}\")\nprint(f\"Mode: {'Single' if ts_model.single_sufficiency else 'Multi'} Sufficiency\")\n</pre> # ============================================================ # Prepare data for TwoStageModel # ============================================================ from two_stage.model import find_bins_intervals  # Add 'observed' column (required by TwoStageModel) df['observed'] = df[TARGET_COL]  # Extract model name from prediction column (e.g., 'predicted_linear' -&gt; 'linear') MODEL_NAME = PRED_COL.replace('predicted_', '')  # Create bins intervals for density and sufficiency bins_intervals = find_bins_intervals(df, density_bins=7) print(f\"Created bins for {len(bins_intervals[1])} sufficiency levels\")  # ============================================================ # Initialize and fit TwoStageModel # ============================================================ ts_model = TwoStageModel(     spline=7,           # GAM spline knots     lam=0.5,            # GAM regularization     resolution=[30, 30] # Spatial aggregation grid )  # Fit model using test data to analyze accuracy patterns # This learns how accuracy varies with density and location # In real case, you should use the density based on training stations ts_model.fit(     df_train_raw=df.loc[test_idx],     model_name=MODEL_NAME,     bins_intervals=bins_intervals,     split_by='grid' )  print(f\"\\n{'='*50}\") print(f\"TwoStageModel Training Complete!\") print(f\"{'='*50}\") print(f\"Model: {PRED_COL} \u2192 {TARGET_COL}\") print(f\"Stage 1 R\u00b2 (GAM on density): {ts_model.stage1_score:.4f}\") print(f\"Stage 2 R\u00b2 (GAM+SVM spatial): {ts_model.stage2_score:.4f}\") print(f\"Mode: {'Single' if ts_model.single_sufficiency else 'Multi'} Sufficiency\")  <pre>Created bins for 1 sufficiency levels\n\n==================================================\nTwoStageModel Training Complete!\n==================================================\nModel: predicted_linear \u2192 Ozone\nStage 1 R\u00b2 (GAM on density): 0.9872\nStage 2 R\u00b2 (GAM+SVM spatial): 0.6429\nMode: Single Sufficiency\n</pre> In\u00a0[97]: Copied! <pre># Your network stations and sample size\nstations = df.drop_duplicates(subset=['longitude', 'latitude'])\nsufficiency = df['sufficiency'].unique()[0]\n\n# Import visualization tools\nfrom two_stage import plot_predicted_accuracy_map, predict_at_locations\n\n# Example 1: Predict at specific locations\nr2_dense, _ = predict_at_locations(ts_model, 5.0, 50.0, stations['longitude'], stations['latitude'], sufficiency)\nr2_sparse, _ = predict_at_locations(ts_model, 30.0, 55.0, stations['longitude'], stations['latitude'], sufficiency)\nprint(f\"Dense area (5\u00b0E, 50\u00b0N):  R\u00b2 = {r2_dense[0]:.4f}\")\nprint(f\"Sparse area (30\u00b0E, 55\u00b0N): R\u00b2 = {r2_sparse[0]:.4f}\")\nprint(f\"Equity Gap: {float(r2_dense[0] - r2_sparse[0]):.4f}\")\n\n# Example 2: Plot predicted accuracy map\nfig, axes = plot_predicted_accuracy_map(\n    ts_model, stations['longitude'], stations['latitude'], sufficiency,\n    lon_range=(-10, 35), lat_range=(35, 70), grid_size=30\n)\nplt.show()\n</pre> # Your network stations and sample size stations = df.drop_duplicates(subset=['longitude', 'latitude']) sufficiency = df['sufficiency'].unique()[0]  # Import visualization tools from two_stage import plot_predicted_accuracy_map, predict_at_locations  # Example 1: Predict at specific locations r2_dense, _ = predict_at_locations(ts_model, 5.0, 50.0, stations['longitude'], stations['latitude'], sufficiency) r2_sparse, _ = predict_at_locations(ts_model, 30.0, 55.0, stations['longitude'], stations['latitude'], sufficiency) print(f\"Dense area (5\u00b0E, 50\u00b0N):  R\u00b2 = {r2_dense[0]:.4f}\") print(f\"Sparse area (30\u00b0E, 55\u00b0N): R\u00b2 = {r2_sparse[0]:.4f}\") print(f\"Equity Gap: {float(r2_dense[0] - r2_sparse[0]):.4f}\")  # Example 2: Plot predicted accuracy map fig, axes = plot_predicted_accuracy_map(     ts_model, stations['longitude'], stations['latitude'], sufficiency,     lon_range=(-10, 35), lat_range=(35, 70), grid_size=30 ) plt.show()  <pre>Dense area (5\u00b0E, 50\u00b0N):  R\u00b2 = 0.6148\nSparse area (30\u00b0E, 55\u00b0N): R\u00b2 = 0.5680\nEquity Gap: 0.0468\n</pre> In\u00a0[98]: Copied! <pre># Generate diagnostics\ndiagnosis = ts_model.diagnose(save_dir='diagnostics/', show=True)\n\nprint(\"\\nDiagnostic files saved:\")\nprint(\"  - diagnostics/stage1_gam.png\")\nprint(\"  - diagnostics/stage2_svm.png\")\nprint(\"  - diagnostics/diagnosis.txt\")\n</pre> # Generate diagnostics diagnosis = ts_model.diagnose(save_dir='diagnostics/', show=True)  print(\"\\nDiagnostic files saved:\") print(\"  - diagnostics/stage1_gam.png\") print(\"  - diagnostics/stage2_svm.png\") print(\"  - diagnostics/diagnosis.txt\")  <pre>\u2713 Stage 1 diagram saved to: diagnostics/stage1_gam.png\n</pre> <pre>\u2713 Stage 2 diagram saved to: diagnostics/stage2_svm.png\n</pre> <pre>\u2713 Diagnosis saved to: diagnostics//\n\nDiagnostic files saved:\n  - diagnostics//stage1_gam.png\n  - diagnostics//stage2_svm.png\n  - diagnostics//diagnosis.txt\n\n========== TwoStageModel Diagnosis ==========\n\nMode: Single Sufficiency\n\nStage 1 (Monotonic GAM) - Density/Sampling Effect:\n  - r = 0.9872, MAE = 0.0214\n  - Training samples: 7\n  - Spline knots: 7, Lambda: 0.5\n\nStage 2 (SVM) - Spatial Residual:\n  - r = 0.4986, MAE = 0.1830\n  - Training samples: 152\n  - Spatial features: ['longitude', 'latitude']\n  - Resolution: [30, 30]\n\nTotal (GAM + SVM):\n  - r = 0.6429, MAE = 0.1830\n\nKey Insights:\n  1. Density effect captured in Stage 1 (r=0.987)\n     \u2192 Predicts accuracy variation from sampling density\n  2. Spatial pattern captured in Stage 2 (r=0.499)\n     \u2192 Predicts location-specific residuals\n  3. Prediction ability:\n     \u2192 Unseen sampling density: r=0.987 (Stage 1)\n     \u2192 Unseen locations: r=0.643 (Total)\n\n=============================================\n\n\nDiagnostic files saved:\n  - diagnostics/stage1_gam.png\n  - diagnostics/stage2_svm.png\n  - diagnostics/diagnosis.txt\n</pre>"},{"location":"examples/Standard_workflow_geoequity/#twostagemodel-spatial-equity-diagnostic","title":"TwoStageModel: Spatial Equity Diagnostic\u00b6","text":"<p>This notebook demonstrates the complete workflow for diagnosing spatial accuracy patterns using <code>TwoStageModel</code>.</p>"},{"location":"examples/Standard_workflow_geoequity/#two-experiment-modes","title":"Two Experiment Modes\u00b6","text":"Mode Description Use Case Single Sufficiency One fixed sample size Quick analysis, production models Multi Sufficiency Multiple sample sizes (5K, 20K, 100K) Studying how accuracy changes with data volume"},{"location":"examples/Standard_workflow_geoequity/#overview","title":"Overview\u00b6","text":"<ol> <li>Configuration - Set data path, features, target</li> <li>Sufficiency Levels - Single or multi-sufficiency experiment</li> <li>Data Density - Calculate station density</li> <li>Data Split - Site-wise cross-validation</li> <li>Feature Engineering - Spatial/temporal harmonics (customizable!)</li> <li>Train Model - Baseline ML model</li> <li>TwoStageModel - Fit &amp; diagnose</li> <li>Predict - Accuracy at new locations</li> </ol>"},{"location":"examples/Standard_workflow_geoequity/#1-configuration-load-data","title":"1. Configuration &amp; Load Data\u00b6","text":"<p>Modify the configuration below to use your own dataset!</p>"},{"location":"examples/Standard_workflow_geoequity/#required-columns","title":"Required columns:\u00b6","text":"<ul> <li><code>longitude</code>, <code>latitude</code>: Spatial coordinates (float)</li> <li><code>time</code>: Timestamp (datetime)</li> <li>Target column (e.g., <code>o3</code>): Observed values (float)</li> </ul>"},{"location":"examples/Standard_workflow_geoequity/#configuration-variables","title":"Configuration variables:\u00b6","text":"Variable Description <code>DATA_PATH</code> Path to your pickle file <code>TARGET_COL</code> Column name for target values <code>FEATURE_COLS</code> List of feature column names <code>PRED_COL</code> Name for prediction column <p>The example uses ozone observations from June-September 2019 (3.5M samples).</p>"},{"location":"examples/Standard_workflow_geoequity/#11-create-sufficiency-levels","title":"1.1 Create Sufficiency Levels\u00b6","text":"<p>Single mode: Use full dataset as one sufficiency level (100k).</p> <p>Multi mode: Sample data at 3 sizes (5K, 20K, 100K) to study how accuracy varies with data volume.</p>"},{"location":"examples/Standard_workflow_geoequity/#2-calculate-data-density","title":"2. Calculate Data Density\u00b6","text":"<p>Calculate data density for each location. Higher density = more nearby observation stations.</p>"},{"location":"examples/Standard_workflow_geoequity/#3-split-data-site-wise-cross-validation","title":"3. Split Data (Site-wise Cross-Validation)\u00b6","text":"<p>Use site-wise split to avoid data leakage - same station's observations stay in same set.</p>"},{"location":"examples/Standard_workflow_geoequity/#4-feature-engineering-train-model","title":"4. Feature Engineering &amp; Train Model\u00b6","text":""},{"location":"examples/Standard_workflow_geoequity/#feature-engineering-options","title":"Feature Engineering Options\u00b6","text":"<p>You can use your own feature engineering method! The provided <code>simple_feature_engineering</code> is just an example.</p> Step Description Fill NA Replace missing values with median Temporal Harmonics Add T1, T2, T3 (Fourier features for seasonality) Spatial Harmonics Add S1, S2, S3 (spherical harmonics from lon/lat) Standardize Zero mean, unit variance scaling <pre># Option A: Use provided feature engineering\nX, y, feature_names = simple_feature_engineering(df, FEATURE_COLS, TARGET_COL)\n\n# Option B: Use your own method\nX = your_custom_preprocessing(df[FEATURE_COLS])\ny = df[TARGET_COL]\n</pre>"},{"location":"examples/Standard_workflow_geoequity/#5-fit-twostagemodel","title":"5. Fit TwoStageModel\u00b6","text":"<p>Analyze how accuracy varies with data density and location.</p>"},{"location":"examples/Standard_workflow_geoequity/#6-predict-accuracy-for-new-locations","title":"6. Predict Accuracy for New Locations\u00b6","text":"<p>Use the fitted model to predict expected accuracy (R\u00b2) at any location.</p> <p>Key Points:</p> <ul> <li><code>density</code> must be calculated per-pixel (based on distance to training stations)</li> <li>For multi-sufficiency mode, we use the first sufficiency level as default</li> <li>Predictions use the new API: <code>predict(longitude, latitude, density, sufficiency)</code></li> </ul>"},{"location":"examples/Standard_workflow_geoequity/#7-generate-diagnostic-report","title":"7. Generate Diagnostic Report\u00b6","text":"<p>Generate diagnostic plots and text report for model analysis.</p>"},{"location":"examples/Standard_workflow_geoequity/#summary","title":"Summary\u00b6","text":""},{"location":"examples/Standard_workflow_geoequity/#core-concepts","title":"Core Concepts\u00b6","text":"Term Definition Sufficiency Training sample size (e.g., 5K, 20K, 100K observations) Density Observation station density at a location (stations/km\u00b2 within radius)"},{"location":"examples/Standard_workflow_geoequity/#workflow","title":"Workflow\u00b6","text":"Step Function Description 1 <code>calculate_density()</code> Calculate station density for each location 2 <code>split_test_train()</code> Site-wise split avoiding data leakage 3 <code>TwoStageModel.fit()</code> Learn density\u2192accuracy + spatial patterns 4 <code>TwoStageModel.predict()</code> Predict accuracy at any location 5 <code>TwoStageModel.diagnose()</code> Generate diagnostic plots &amp; report"},{"location":"examples/Standard_workflow_geoequity/#key-insight","title":"Key Insight\u00b6","text":"<p>Machine learning models trained on geospatial data exhibit spatial inequity: prediction accuracy varies systematically across space. This inequity arises from two sources:</p> <ol> <li><p>Sampling Density Effect (Stage 1): Regions with denser observation networks provide more training data, leading to higher local accuracy. The monotonic GAM captures this density\u2192accuracy relationship.</p> </li> <li><p>Spatial Residual Effect (Stage 2): Even after accounting for density, some locations have systematically higher/lower accuracy due to local data quality, terrain complexity, or other spatial factors. The SVM captures these location-specific patterns.</p> </li> </ol> <p>TwoStageModel quantifies and predicts this spatial inequity, enabling:</p> <ul> <li>Fair assessment of model performance across regions</li> <li>Identification of under-served areas needing more observations</li> <li>Uncertainty quantification for downstream applications</li> </ul>"},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#from-pypi","title":"From PyPI","text":"<pre><code>pip install geoequity\n</code></pre>"},{"location":"getting-started/installation/#from-source","title":"From Source","text":"<pre><code>git clone https://github.com/px39n/geoequity.git\ncd geoequity\npip install -e .\n</code></pre>"},{"location":"getting-started/installation/#optional-dependencies","title":"Optional Dependencies","text":"<p>For full functionality including GAM models and geospatial visualization:</p> <pre><code>pip install geoequity[full]\n</code></pre> <p>This installs: - <code>pygam</code> - For Generalized Additive Models - <code>xarray</code> - For gridded data handling - <code>geopandas</code> - For geospatial data operations</p>"},{"location":"getting-started/installation/#requirements","title":"Requirements","text":"<ul> <li>Python &gt;= 3.8</li> <li>numpy &gt;= 1.20</li> <li>pandas &gt;= 1.3</li> <li>scikit-learn &gt;= 1.0</li> <li>matplotlib &gt;= 3.4</li> <li>tqdm &gt;= 4.60</li> </ul>"},{"location":"getting-started/installation/#verify-installation","title":"Verify Installation","text":"<pre><code>import geoequity\nprint(geoequity.__version__)\n</code></pre>"},{"location":"getting-started/quickstart/","title":"Quick Start","text":"<p>This guide walks you through the basic workflow for assessing spatial equity in your ML models.</p>"},{"location":"getting-started/quickstart/#1-load-your-data","title":"1. Load Your Data","text":"<p>Your data should be a pandas DataFrame with: - <code>longitude</code>, <code>latitude</code> columns - A column with your model's predictions - Ground truth values for calculating accuracy</p> <pre><code>import pandas as pd\ndf = pd.read_pickle('your_data.pkl')\n</code></pre>"},{"location":"getting-started/quickstart/#2-calculate-density","title":"2. Calculate Density","text":"<p>Density measures data availability around each observation:</p> <pre><code>from geoequity.data import calculate_density\n\ndf = calculate_density(df, radius=500)  # 500 km search radius\nprint(f\"Density range: {df['density'].min():.2e} - {df['density'].max():.2e}\")\n</code></pre>"},{"location":"getting-started/quickstart/#3-split-data","title":"3. Split Data","text":"<p>Use site-wise splitting to avoid data leakage:</p> <pre><code>from geoequity.data import split_test_train\n\ntrain_sites, test_sites, train_idx, test_idx, df = split_test_train(\n    df, split=0.2, flag='Site', seed=42\n)\n</code></pre>"},{"location":"getting-started/quickstart/#4-fit-twostagemodel","title":"4. Fit TwoStageModel","text":"<p>After training your ML model and adding predictions to <code>df</code>:</p> <pre><code>from geoequity import TwoStageModel\n\nts = TwoStageModel(resolution=[30, 30])\nts.fit(df.loc[test_idx], model_col='predicted_mymodel')\n</code></pre>"},{"location":"getting-started/quickstart/#5-analyze-results","title":"5. Analyze Results","text":"<pre><code># Predict accuracy at any location\naccuracy = ts.predict(longitude=5.0, latitude=50.0, density=0.001)\n\n# Generate diagnostic plots\nts.diagnose(save_dir='diagnostics/')\n</code></pre>"},{"location":"getting-started/quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Learn more about the Two-Stage Model</li> <li>Explore data splitting strategies</li> <li>See the complete example notebook</li> </ul>"},{"location":"guide/splitting/","title":"Data Splitting","text":"<p>Proper data splitting is crucial for valid spatial model evaluation. GeoEquity provides several strategies to prevent data leakage.</p>"},{"location":"guide/splitting/#splitting-strategies","title":"Splitting Strategies","text":""},{"location":"guide/splitting/#site-wise-splitting","title":"Site-wise Splitting","text":"<p>Ensures all observations from the same monitoring site stay together:</p> <pre><code>from geoequity.data import split_test_train\n\ntrain_sites, test_sites, train_idx, test_idx, df = split_test_train(\n    df, split=0.2, flag='Site', seed=42\n)\n</code></pre> <p>Use when: You have repeated measurements at fixed locations.</p>"},{"location":"guide/splitting/#grid-wise-splitting","title":"Grid-wise Splitting","text":"<p>Divides the spatial domain into a grid and assigns entire grid cells to train/test:</p> <pre><code>_, _, train_idx, test_idx, df = split_test_train(\n    df, split=0.2, flag='Grid', grid_size=10, seed=42\n)\n</code></pre> <p>Use when: You want to test spatial extrapolation ability.</p>"},{"location":"guide/splitting/#spatiotemporal-splitting","title":"Spatiotemporal Splitting","text":"<p>Prevents leakage in both space and time:</p> <pre><code>_, _, train_idx, test_idx, df = split_test_train(\n    df, split=0.2, flag='Spatiotemporal', seed=42\n)\n</code></pre> <p>Use when: You have time series at multiple locations.</p>"},{"location":"guide/splitting/#cross-validation","title":"Cross-Validation","text":"<p>All strategies support k-fold cross-validation:</p> <pre><code>train_list, test_list, train_idx_list, test_idx_list, df = split_test_train(\n    df, cv=5, flag='Site', seed=42\n)\n\nfor fold, (train_idx, test_idx) in enumerate(zip(train_idx_list, test_idx_list)):\n    print(f\"Fold {fold}: {len(train_idx)} train, {len(test_idx)} test\")\n</code></pre>"},{"location":"guide/splitting/#why-spatial-splitting-matters","title":"Why Spatial Splitting Matters","text":"<p>Standard random splitting can cause:</p> <ul> <li>Spatial autocorrelation leakage: Nearby points in train/test sets</li> <li>Overly optimistic accuracy: Model appears better than it is</li> <li>Poor generalization: Fails on truly unseen regions</li> </ul> <p>Site-wise and grid-wise splitting prevent these issues by ensuring spatial separation between train and test sets.</p>"},{"location":"guide/two-stage/","title":"Two-Stage Model","text":"<p>The Two-Stage Model is GeoEquity's core method for predicting and understanding spatial variations in model accuracy.</p>"},{"location":"guide/two-stage/#how-it-works","title":"How It Works","text":""},{"location":"guide/two-stage/#stage-1-density-based-accuracy","title":"Stage 1: Density-Based Accuracy","text":"<p>The first stage uses a Generalized Additive Model (GAM) to capture the relationship between data density and model accuracy.</p> <ul> <li>Input: Density features (sparsity, sufficiency)</li> <li>Model: Monotonic GAM with shape constraints</li> <li>Output: Baseline accuracy prediction</li> </ul>"},{"location":"guide/two-stage/#stage-2-spatial-residuals","title":"Stage 2: Spatial Residuals","text":"<p>The second stage uses a Support Vector Machine (SVM) to capture spatial patterns in the residuals from Stage 1.</p> <ul> <li>Input: Spatial coordinates (longitude, latitude)</li> <li>Model: SVM with RBF kernel</li> <li>Output: Spatial adjustment to accuracy</li> </ul>"},{"location":"guide/two-stage/#usage","title":"Usage","text":"<pre><code>from geoequity import TwoStageModel\n\n# Initialize with custom parameters\nmodel = TwoStageModel(\n    spline=7,           # Number of spline bases for GAM\n    lam=0.5,            # GAM regularization\n    resolution=[30, 30] # Grid resolution for SVM\n)\n\n# Fit the model\nmodel.fit(\n    df,                      # DataFrame with test data\n    model_col='predicted',   # Column with model predictions\n    target_col='observed'    # Column with ground truth (optional)\n)\n\n# Predict accuracy\naccuracy = model.predict(\n    longitude=5.0,\n    latitude=50.0,\n    density=0.001\n)\n\n# Generate diagnostics\nmodel.diagnose(save_dir='output/', show=True)\n</code></pre>"},{"location":"guide/two-stage/#diagnostic-outputs","title":"Diagnostic Outputs","text":"<p>The <code>diagnose()</code> method generates:</p> <ol> <li>stage1_gam.png: GAM partial dependence plots showing density-accuracy relationship</li> <li>stage2_svm.png: Spatial map of residual patterns</li> <li>diagnosis.txt: Summary statistics and model parameters</li> </ol>"},{"location":"guide/two-stage/#interpretation","title":"Interpretation","text":"<ul> <li>High Stage 1 residuals: Spatial factors beyond density affect accuracy</li> <li>Clustered Stage 2 patterns: Geographic regions with systematic over/under-performance</li> <li>Uniform Stage 2: Density alone explains accuracy variations</li> </ul>"},{"location":"guide/visualization/","title":"Visualization","text":"<p>GeoEquity provides tools for creating publication-ready spatial visualizations.</p>"},{"location":"guide/visualization/#accuracy-maps","title":"Accuracy Maps","text":"<p>Visualize predicted accuracy across space:</p> <pre><code>from geoequity.visualization import plot_accuracy_map\n\nfig, ax = plot_accuracy_map(\n    model,\n    ds,                    # xarray Dataset with coordinates\n    mask=land_mask,        # Optional land/sea mask\n    cmap='Spectral_r',\n    vmin=0, vmax=1,\n    title='Model Accuracy'\n)\n</code></pre>"},{"location":"guide/visualization/#diagnostic-plots","title":"Diagnostic Plots","text":"<p>The TwoStageModel includes built-in diagnostic visualization:</p> <pre><code>model.diagnose(save_dir='diagnostics/', show=True)\n</code></pre> <p>This generates: - stage1_gam.png: Partial dependence plots for density features - stage2_svm.png: Spatial map of residual patterns</p>"},{"location":"guide/visualization/#custom-visualizations","title":"Custom Visualizations","text":""},{"location":"guide/visualization/#accuracy-comparison","title":"Accuracy Comparison","text":"<p>Compare multiple models or visualization modes:</p> <pre><code>from geoequity.visualization import plot_accuracy_comparison\n\nfig, axes = plot_accuracy_comparison(\n    df,\n    model_name='MyModel',\n    modes=['observation', 'interpolation', 'spatial_model'],\n    accuracy_range=(0, 1),\n    lon_range=(-10, 35),\n    lat_range=(35, 70)\n)\n</code></pre>"},{"location":"guide/visualization/#density-distribution","title":"Density Distribution","text":"<p>Visualize data density across space:</p> <pre><code>import matplotlib.pyplot as plt\n\nplt.scatter(\n    df['longitude'], df['latitude'],\n    c=df['density'], cmap='viridis',\n    s=5, alpha=0.5\n)\nplt.colorbar(label='Density')\n</code></pre>"},{"location":"guide/visualization/#styling-tips","title":"Styling Tips","text":"<ul> <li>Use <code>Spectral_r</code> or <code>RdYlGn</code> for accuracy (red=low, green=high)</li> <li>Set consistent <code>vmin=0, vmax=1</code> for R\u00b2 values</li> <li>Add land boundaries with geopandas for context</li> </ul>"}]}